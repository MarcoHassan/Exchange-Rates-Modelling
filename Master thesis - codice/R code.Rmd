---
title: "Master thesis code - FX rates. A nonlinear evaluation"
output:  rmarkdown::github_document
---

Libraries used
```{r}
## library("Quandl") ##Offers to connect to download various eco and financial datasets avaialable on quandl.com through the Quandl API. !!At the moment not used!!
library(quantmod) ##same principle as Quandl, not limited by the amount of API calls though and connect to multiple daset providers. In this study we will mainly query the FRED database.
library(forecast) ## for seasonality adjusting the times series
## library(dplyr) ## for data wrangling; not used. not applicable to xts objects.
library(tseries)
```

Specify quandl API
```{r}
## Quandl.api_key("enter api key") #not used quandl at the moment
```


===============
Data Collection
===============

Meese and Rogoff also used the Forward rates for their estimation.

Specify function to get symbols
```{r}
get_Symbols <- function(x){
                           lapply(x, function(sym){getSymbols(sym, src = "FRED",
                                                              return.class = "xts",
                                                              auto.assign = F)})
  ## auto.assign specifies that not the ticker should be passed
}
```

###########
FX - rates
###########

Download foreign exchange rates
```{r}
Symbols <- c("EXUSUK", "EXJPUS", "EXSZUS")

FX <-  get_Symbols(Symbols)

## Invert the USUK rate to UKUS to keep it consistent with the other series
FX[[1]] <- 1/FX[[1]]

FX <- do.call(merge, FX)

colnames(FX)[] <- c("EX_UKUS", "EX_JPUS", "EX_CHUS")

## notice like that all of the evaluation will be performed compared to the usa. if you want to make smaller bilateral comparison you need to download the other forex.
```

###
GDP
###

GDP is an aggregated statistic that is published on a quarterly basis. You can use monthly proxies. Electricity consumption? Industrial production? <-> this is normalized to 100. Is that good?

Unemployment not good. Does not capture the size of an economy. Here it will be important.
```{r}
## use industrial production? 
## use unemployment rate. This approach preferred
Symbols <- c("LRUN64TTUSM156N", "LRUN64TTJPM156N", "LMUNRRTTCHM156N", "LMUNRRTTGBM156N") #F not seasonally adjusted.

UNply <- get_Symbols(Symbols)

UNply <- do.call(merge, UNply) ## notice the unemployment series for UK starts just in the 90s.

colnames(UNply) <- c("UN_US", "UN_JP", "UN_CH", "UN_UK")
```


################
Short term bills
################

Treasury bills for capturing short term attractiveness of investments
```{r}
Symbols <- c("TB3MS","INTGSTJPM193N","TBDRUKM")
 ## 3 months bill rate usa; secondary market.
 ## bill for japan; explaination not clear. check at it again to clearly understand what it is precisely.
 ## treasury bill for uk

bills <- get_Symbols(Symbols)

bills<- do.call(merge, bills) 

colnames(bills) <- c("TB_US", "TB_JP", "TB_CH") ## Treasury bills

## above not available for switzerland. use interbank rate for it.
```


#############
Monetary base
#############

Problem you are taking differentials for all of the above measures. The values should consequently be consistent and comparable. Use M3 or M1 for all; these should be similarly defined.
```{r}
Symbols <- c("MYAGM3USM052N", "MYAGM3JPM189N", "MABMM301GBM189N", "MABMM301CHM189N") #us, jp, uk, not seasonally adjusted. 

M3 <- get_Symbols(Symbols)

M3 <- do.call(merge, M3)

colnames(M3) <- c("M3_US", "M3_JP", "M3_UK", "M3_CH")

## all the values in national currency. Will need to transform them at a later period using FX-rate and then take the logarithm.

```

Extract M1
```{r}
Symbols <- c("M1NS", "MANMM101JPM189N", "MANMM101GBM189N", "MANMM101CHM189N") 

M1 <- get_Symbols(Symbols)

M1 <- do.call(merge, M1)

colnames(M1) <- c("M1_US", "M1_JP", "M1_UK", "M1_CH")


## all the values in national currency. Will need to transform them at a later period using FX-rate and then take the logarithm.
```

###
CPI
###

```{r}
## all not seasonally adjusted, all with basis 2015
Symbols <- c('CHECPIALLMINMEI', 'JPNCPIALLMINMEI', 'GBRCPIALLMINMEI', 'USACPIALLMINMEI')

CPI <- get_Symbols(Symbols)

CPI <- do.call(merge, CPI)

colnames(CPI) <- c("CPI_CH", "CPI_JP", "CPI_UK", "CPI_US")
  
## Important difference, lapply applies a function over a list, do.call calls a function with a list of arguments.

## CHECPIALLMINMEI ## switzerland from FRED; harmonised index; basis 2015
## JPNCPIALLMINMEI ## japan; basis 2015
## GBRCPIALLMINMEI ## uk; basis 2015
## USACPIALLMINMEI ## usa, basis 2015

## good all with the same basis
```

##############
Trade balances
##############

```{r}
## Current accounts are available just on quartal level. Two options use current accounts and hold it fix for the entire quartal.

## Option two; chosen option, find a proxy for the current account balance.

Symbols <- c("XTNTVA01CHM664N", "XTNTVA01JPM664N", "XTNTVA01USM664N", "XTNTVA01GBM664N")
 ## net trade. monthly not seasonally adjusted. broad proxy for current account balance.

Trade <- get_Symbols(Symbols)

Trade <- do.call(merge, Trade)

colnames(Trade) <- c("T_CH", "T_JP", "T_US", "T_UK") 

## check at the currency. All in national currency! Convert them. 
```


###########################
Merge all of the statistics
###########################

```{r}
data <- merge.xts(FX, bills, CPI, M1, M3, Trade, UNply) ##dagli anni '90 al 2005 hai tutte le statistiche.

rm(list=setdiff(ls(), "data")) ## clean the workspace just keeping the data

data <- data["1990-01-01/2006-02-01"] ## extract a complete dataset without NA; notice if you run the analysis with M1 you have complete data until 2013.

length(data[,1]) ## 194 observations per ts

## Transform all the variables in USD currency
```

UK statistics are poor. For the other countries you can run the analysis back to the '70s. For UK because of M3 and especially unemployment rate the comparison is much more limited.

###################
Data transformation
###################

Transform M1, M3 and Trade balance sheets in USD. Compute cumulative Trade balance sheet.
Transform also the interest rate in USD interest rate. Assume FX-rate martingale. Otherwise you would have to download the forward interest rate. 
```{r}
helper1 <- c("M1", "M3", "TB", "T")
for(j in helper1){
                  helper2 <- c("JP", "CH", "UK")
                  help <- function(x){eval(parse(text = x))}
                  for (i in helper2){
                       if(paste0(j, "_", i) != "TB_UK")
                       data[,paste0(j, "_", i)] <-help(paste0("data$", j, "_", i))/help(
                                                              paste0("data$EX_", i, "US"))
                  }
}
rm(helper1)
rm(list = setdiff(ls(), "data"))
  
##Explaination of the code above; a loop in a loop to paste the code the right way and convert all of the variables into USD values. This will facilitate the comparison and the times series analysis at a later step.
```

Transform Trade to cumulative trade variable
```{r}
helper2 <- c("JP", "CH", "UK", "US")
for(j in helper2){
    total = 0
    for(i in 1:length(data$T_CH)){
      total = total + as.numeric(data[i, paste0("T_", j)])
      data[i, paste0("T_", j)] <- total
    }
    
}
rm(list = setdiff(ls(), "data"))
```

Logarithm transformation
```{r}
transformation <- c("M1", "M3", "EX")
helper2 <- c("JP", "CH", "UK", "US")
for(j in transformation){
    for(i in helper2){
      if(j != "EX"){
         data[, paste0(j, "_", i)] <- log(data[, paste0(j, "_", i)])
      }
      else{if(i != "US") data[, paste0(j, "_", i, "US")] <- log(data[, paste0(j, "_", i,
                                                                              "US")])}
    }
    
}
rm(list = setdiff(ls(), "data"))
```

======================
Descriptive Statistics
======================

This section continues with general descriptive statistics trying to understand the behaviour of the exogenous series and trying to test whether the latter are indeed exogenous.

Before diving into the descriptive analysis of the series, I will try to transform the series into stationary series so that it will possible to analyze the various series throught linear and non-linear times series models.

In order to transform the series into stationary series I will:

    1. Detrend the series.

    2. Remove the seasonal component.
    
    3. Test the series for unit roots.
    
    4. Perform chow-tests to check for structural breaks in the resulting series.
    
    
    
##################
Part 1: Detrending
##################

In a first step I will test the necessity for detrending the series.

Plot the series as a help:
```{r}
help <- function(i){
  plot(i, main = colnames(i))
}
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if((i == 1) || (i == 4))
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])
}
```


Plot proxy for variance as a help:
```{r}
help <- function(i)plot((i- mean(i))**2, main = colnames(i)) ## notice assumes time invariant mean, which is clearly violated in the series.
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if(i == 1)
    par(mfrow = c(3,1))
  if(i == 4)
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])                  
}
```

All the series are clearly not mean stationary. 

Just for the exchange rate it is difficult to judge based on simple plots, these might could appear to be mean stationary but not volatility stationary. 
```{r}
help <- cbind(data$EX_UKUS, data$EX_JPUS, data$EX_CHUS)

par(mfrow = c(2,2))
lapply(help, acf) ## slow decaying autocorrelation points to non-stationarity mean series.
## The slow decay suggests that the series would be mean varyiant based on time shifts.
```


Step 2: Choose detrending solution

(i) Detrend via differenciation

(ii) Detrend via linear time trend

(iii) Use a moving average filter, this is a 'low pass' filter since it takes data and removes from it the rapidly fluctuating component  to leave the slowly varying estimated
trend.

Linear trend will provide to be ineffective for all of the times series.


Perform detrending via differentiation
```{r}
data2 <- data ## to play then with original dataset
data3 <- lapply(data, diff)
for(i in 1:ncol(data)){data[,i] <- data3[[i]]}
```


Plot new Variables
```{r}
help <- function(i){
  plot(i, main = colnames(i))
}
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if((i == 1) || (i == 4))
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])
}
```

Plot proxy for variance for the new variables as a help:
```{r}
help <- function(i)plot((i- mean(i, na.rm = T))**2, main = colnames(i)) ## notice assumes time invariant mean, which is clearly violated in the series.
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if(i == 1)
    par(mfrow = c(3,1))
  if(i == 4)
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])                  
}
```


Plot autocorrelation functions
```{r}
for(i in 1:ncol(data)){
  if(i == 1)
    par(mfrow = c(3,1), mar = c(4,4,4,2))
  if(i == 4)
    par(mfrow = c(3,1), mar = c(4,4,4,2))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2), mar = c(4,4,4,2))
  acf(na.trim(data[,i]), main = colnames(data[,i]))                 
}
```


Result: Trade was a disaster. For the rest good improvement. Some adjustment still needed - see TB_US; possibly double differentiation. 

Compute moving average smoother for Trade.
```{r}
helper1 <- c("JP", "CH", "UK", "US")

par(mfrow = c(2,2), mar = c(4,4,4,2))
for(i in helper1){
          trend <- ma(data2[, paste0("T_", i)], 
                                       order = 4, centre = T) ##moving average of quartal
          data[, paste0("T_", i)] <- as.ts(data2[, paste0("T_", i)]) - trend
          acf(na.trim(data[,paste0("T_", i)]), main = paste0("T_", i)) 
         
}
```


Check for unit roots in the series performing augmented Dickey-Fuller tests
```{r}
help <- function(x){
        dd <- adf.test(na.trim(x))
        if(dd$p.value >= 0.05) dd
}

suppressWarnings(lapply(data, help)) ## supress warning to ignore message that true p-value smaller than the one printed
```


From the above two problematic series:

1. Treasury bills of US.

2. Unemployment rate of UK.

Solution: double differentiate the both
```{r}
data$TB_US <- diff(data$TB_US)
data$UN_UK <- diff(data$UN_UK)
acf(na.trim(data$TB_US))
acf(na.trim(data$UN_UK))

```

Will all of the above corrections the reslut is good and there is strong evidence for stationarity in the series.

Clean workspace and just keep the full dataset
```{r}
data <- na.trim(data)
rm(list=setdiff(ls(), "data"))
```


Step 3: analyze seasonality and correct for it.

Two possible solutions for adjusting for seasonality after identification of the cyclical frequency:

(i) Seasonal differentiation. Similar idea to the general differentiation of a series.

(ii) Subtract the block average of the series in the cycle. Similar idea to moving average smoothers.

Let's test now for the presence of seasonal patterns and let's try to identify the cyclical frequency. 

Checking again at autocorrelation plots
```{r}

```

Perform FFT to check at the sesaonality frequency
```{r}

```












































Idea transform perform ADF to check which of the series are still troublesome. 

These should be able to capture the seasonal tred by seasonal differencing and refine the results above by incorporating higher order differences.

Augmented dickey fuller test

Use moving average smoother for the Trade balance. 


```{r}
nsdiffs(ts(data$UN_US, frequency = 12)) ## has unit root
acf(na.trim(diff(data$TB_US, 1)))
acf(data$TB_US)
adf.test(na.trim(diff(data$TB_US)))
```



Step 2: Seasonality
In order to derive meaningful conclusions in the following sections it is necessary to assure that the series do not display profund autocorrelation. This would in fact inhibit the idependency of the data and would bias all of the resulting statistics.
































#################### BELOW NOT GOOD ####################################

Step 1.1: Check for underlying trend in the series. 
```{r}
Season <- function(t_series,  idx){
  trend <- ma(na.trim(t_series), order = 12, centre = T)
  Seasonality <- as.ts(na.trim(t_series)) - trend
  Seasonality <- na.contiguous(Seasonality)
  if(idx == 1) plot(Seasonality)
  if(idx == 2) acf(Seasonality) 
  if(idx == 3) pacf(Seasonality)
  if(idx == 4){
    converted_ts <- ts(na.trim(t_series), frequency = 12)
    decompose_ts <- decompose(converted_ts, "additive")
    adjust_ts <- converted_ts - decompose_ts$seasonal
    plot(adjust_ts)
  }
}

helper <- c("US", "JP", "CH", "UK")
par(mfrow=c(2,2))
for (i in helper){Season(eval(parse(text = paste0("data$UN_", i))), 3)} ##significant autoregressive lags
```


######### repetition of code for other random variables BAD ################### 


Do similar for other functions and incorporate all of the code in one single function if you want to be fancy
```{r}
par(mfrow=c(2,2))
for (i in helper){Season(eval(parse(text = paste0("data$M1_", i))), 2)} ## also for M1 and M3 profound seasonality
```

CPI
```{r}
par(mfrow=c(2,2))
for (i in helper){Season(eval(parse(text = paste0("data$CPI_", i))), 2)} ## also for CPI profound seasonality
```

Test 
```{r}
acf(na.trim(data$EX_CHUS)) ## this is in fact the reasoning of the high performance of random walk model. Very big difference between FX-rates and returns in this sense.
```

Remove helper and i
```{r}
rm(list = c("helper", "i"))
```


###################################################################################






Create seasonally adjusted times series


Check trend stationary vs difference stationary times series

Trend stationary are not mean stationary but include a trend.

Difference stationary contain a stochastic trend, i.e. are non-stationary in the variance
component so that with the length of forecasting horizon the uncertainty
increases to endless.

a time series is called integrated of order d,if after d differencing
the series follows a stable and invertible ARMA process and thus an
I????????0???? process; the property of ????????0???? implies stationarity, whereas the reverse
does not hold.

So try to model SARIMA models and check if after differencitation and integration the series follows a stable and inveritbe ARMA process.

Test for trend stationary times series
```{r}

```




### Remember to do chow-test on residuals
















```{r}
deseason <- function(x){trend <- ma(na.trim(x), order = 4, centre = T)
                        detrended <- as.ts(na.trim(x)) - trend
                        converted_ts <- ts(detrended, frequency = 4)
                        decompose_ts <- decompose(converted_ts, "additive")
                        return(converted_ts - decompose_ts$seasonal)}

deseason2 <- function(x){trend <- ma(na.trim(x), order = 4, centre = T)
                        detrended <- as.ts(na.trim(x)) - trend}

## View(deseason(data$UN_CH)) ## loose some obvservations when taking moving averages.

data2 <- data
for(i in 1:26){data2[,i] <- deseason(data[,i])}

data2 <- na.contiguous(data2)

data3 <- data
for(i in 1:26){data3[,i] <- deseason2(data[,i])}
data3 <- na.contiguous(data3)

```

Autocorrelation plot of original vs. cleaned times series
```{r}
plot(diff(data$M3_US,1)) ### for money stock clear trend. differentiation necessary.
par(mfrow = c(1,2), mar = c(4,4,4,2))
acf(data2$M3_US) ## it seams that some seasonality is still present
acf(data$M3_US)
acf(data3$M3_US)

Box.test(data2$M3_US, lag = 5, type = "Ljung-Box") ## enough evidence that the series not stationary.
```


Decompose cannot really captures seasonality. probably not on frequency 12. Try to get frequency via FFT
```{r}
library(TSA)
p = periodogram(data$EX_UKUS) ## short lag determinant

dd = data.frame(freq=p$freq, spec=p$spec)
order = dd[order(-dd$spec),]
top2 = head(order, 4)
 
# display the 2 highest "power" frequencies
top2

# convert frequency to time periods
time = 1/top2$f
time

## seasonality is not annual but rather trimestral and semestral!!
```


Try with SARIMA, that is try to solve the seasonality problem using differentatiation and ARMA modelling of seasonality.
This will result in a seasonally integrated non-stationary model.
```{r}
# pretty cool code:
nd_pars<-expand.grid(ar=0:2,diff=0:1,ma=0:2,sar=0:1,sdiff=0:1,sma=0:1)
nd_aic<-rep(0,nrow(nd_pars))
for(i in seq(along=nd_aic)){
                            nd_aic[i]<-AIC(arima(data2$EX_UKUS,unlist(nd_pars[i,1:3]),
                                                 unlist(nd_pars[i,4:6])
                                                 ),
                                           k=log(length(data2$EX_UKUS))
                                           )
}
nd_pars[which.min(nd_aic),]
# -> should estimate SARIMA(1,0,2)(0,0,1)

UKUS_arima<-arima(data2$EX_UKUS,order=c(1,0,2), seasonal= list(order = c(0,0,1), period = 3))
UKUS_arima
par(mar=c(5,4,4,2))
tsdiag(UKUS_arima) 

## tsdiag to check diagnostic plots of linear models such ARIMA.

## sarima performs well at 
```

Ma filter
```{r}
ma5=stats::filter(data$EX_UKUS, sides=2,2/5)
plot(ma5)
View(ma5)
```


================================
Univariate times series analysis
================================

White noise prediction
```{r}

```




