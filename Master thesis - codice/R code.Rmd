---
title: "Master thesis code - FX rates. A nonlinear evaluation"
output:  rmarkdown::github_document
---

Libraries used
```{r}

## String manipulation library using regualr expressions

library(stringr) ## to make use of regular expressions to parse strings

## Data collection library

library(quantmod) ##same principle as Quandl, not limited by the amount of API calls though and connect to multiple daset providers. In this study we will mainly query the FRED database and extract data by referring to their API.

## Data cleaning libraries

library(forecast) ## for seasonality adjusting the times series
library(tseries) ## -> check when used
library(TSA) ## for applying FFT to get seasonality frequencies
library(strucchange) ## to perform chow test

## VAR packages

library(vars)  ## to estimate VAR lag with exogenous variables
library(tsDyn) ## to estimate the VAR model and perform the out of sample fit based on rolling window.

## Import self defined functions
source("functions.R")
```


===============
Data Collection
===============

###########
FX - rates
###########

Download foreign exchange rates
```{r}
Symbols <- c("EXUSUK", "EXJPUS", "EXSZUS")

FX <-  get_Symbols(Symbols)

## Invert the USUK rate to UKUS to keep it consistent with the other series
FX[[1]] <- 1/FX[[1]]

FX <- do.call(merge, FX)

colnames(FX)[] <- c("EX_UKUS", "EX_JPUS", "EX_CHUS")

## notice like that all of the evaluation will be performed compared to the usa. if you want to make smaller bilateral comparison you need to download the other forex.
```

###
GDP
###

GDP is an aggregated statistic that is published on a quarterly basis. You can use monthly proxies. Electricity consumption? Industrial production? <-> this is normalized to 100. Is that good?

Unemployment not good. Does not capture the size of an economy. Here it will be important.
```{r}
## use industrial production? 
## use unemployment rate. This approach preferred
Symbols <- c("LRUN64TTUSM156N", "LRUN64TTJPM156N", "LMUNRRTTCHM156N", "LMUNRRTTGBM156N") #F not seasonally adjusted.

UNply <- get_Symbols(Symbols)

UNply <- do.call(merge, UNply) ## notice the unemployment series for UK starts just in the 90s.

colnames(UNply) <- c("UN_US", "UN_JP", "UN_CH", "UN_UK")
```


################
Short term bills
################

Treasury bills for capturing short term attractiveness of investments
```{r}
Symbols <- c("TB3MS","INTGSTJPM193N","TBDRUKM")
 ## 3 months bill rate usa; secondary market.
 ## bill for japan; explaination not clear. check at it again to clearly understand what it is precisely.
 ## treasury bill for uk

bills <- get_Symbols(Symbols)

bills<- do.call(merge, bills) 

colnames(bills) <- c("TB_US", "TB_JP", "TB_CH") ## Treasury bills

## above not available for switzerland. use interbank rate for it.
```

Use interbank for the moment
```{r}
#Symbols <- c("IR3TIB01CHM156N","IR3TIB01USM156N","IR3TIB01GBM156N", "IR3TIB01JPM156N")
 ## 3 months bill rate usa; secondary market.
 ## bill for japan; explaination not clear. check at it again to clearly understand what it is precisely.
 ## treasury bill for uk

#bills <- get_Symbols(Symbols)

#bills <- do.call(merge, bills) 

#colnames(bills) <- c("TB_US", "TB_JP", "TB_CH") ## Treasury bills

```


#############
Monetary base
#############

Problem you are taking differentials for all of the above measures. The values should consequently be consistent and comparable. Use M3 or M1 for all; these should be similarly defined.
```{r}
Symbols <- c("MYAGM3USM052N", "MYAGM3JPM189N", "MABMM301GBM189N", "MABMM301CHM189N") #us, jp, uk, not seasonally adjusted. 

M3 <- get_Symbols(Symbols)

M3 <- do.call(merge, M3)

colnames(M3) <- c("M3_US", "M3_JP", "M3_UK", "M3_CH")

## all the values in national currency. Will need to transform them at a later period using FX-rate and then take the logarithm.

```

Extract M1
```{r}
Symbols <- c("M1NS", "MANMM101JPM189N", "MANMM101GBM189N", "MANMM101CHM189N") 

M1 <- get_Symbols(Symbols)

M1 <- do.call(merge, M1)

colnames(M1) <- c("M1_US", "M1_JP", "M1_UK", "M1_CH")


## all the values in national currency. Will need to transform them at a later period using FX-rate and then take the logarithm.
```

###
CPI
###

```{r}
## all not seasonally adjusted, all with basis 2015
Symbols <- c('CHECPIALLMINMEI', 'JPNCPIALLMINMEI', 'GBRCPIALLMINMEI', 'USACPIALLMINMEI')

CPI <- get_Symbols(Symbols)

CPI <- do.call(merge, CPI)

colnames(CPI) <- c("CPI_CH", "CPI_JP", "CPI_UK", "CPI_US")
  
## Important difference, lapply applies a function over a list, do.call calls a function with a list of arguments.

## CHECPIALLMINMEI ## switzerland from FRED; harmonised index; basis 2015
## JPNCPIALLMINMEI ## japan; basis 2015
## GBRCPIALLMINMEI ## uk; basis 2015
## USACPIALLMINMEI ## usa, basis 2015

## good all with the same basis
```

##############
Trade balances
##############

```{r}
## Current accounts are available just on quartal level. Two options use current accounts and hold it fix for the entire quartal.

## Option two; chosen option, find a proxy for the current account balance.

Symbols <- c("XTNTVA01CHM664N", "XTNTVA01JPM664N", "XTNTVA01USM664N", "XTNTVA01GBM664N")
 ## net trade. monthly not seasonally adjusted. broad proxy for current account balance.

Trade <- get_Symbols(Symbols)

Trade <- do.call(merge, Trade)

colnames(Trade) <- c("T_CH", "T_JP", "T_US", "T_UK") 

## check at the currency. All in national currency! Convert them. 
```


###########################
Merge all of the statistics
###########################

```{r}
data <- merge.xts(FX, bills, CPI, M1, M3, Trade, UNply) ##dagli anni '90 al 2005 hai tutte le statistiche.



data <- data["1990-01-01/2006-02-01"] ## extract a complete dataset without NA; notice if you run the analysis with M1 you have complete data until 2013.

length(data[,1]) ## 194 observations per ts

## Transform all the variables in USD currency

#UK statistics are poor. For the other countries you can run the analysis back to the '70s. For UK because of M3 and especially unemployment rate the comparison is much more limited.
```

Specify a function to clean the environment and do it
```{r}
clean <- function() {
  ENV <- globalenv()
  ll <- ls(envir = ENV)
  ll <- ll[!(ll %in% c("data", "clean", "import", "write", "evalue", "acf_plots",
                       "adfTest", "seasonality", "suspectFrequency", "spa"))]
  rm(list = ll, envir = ENV)
}

clean()
```



###################
Data transformation
###################

Transform M1, M3 and Trade balance sheets in USD. Compute cumulative Trade balance sheet.
Transform also the interest rate in USD interest rate. Assume FX-rate martingale. Otherwise you would have to download the forward interest rate. 
```{r}
series <- c("M1", "M3", "TB", "T")

for(j in series){
                  country <- c("JP", "CH", "UK")
                  for (i in country)
                  {
                  if(paste0(j, "_", i) != "TB_UK")
                       data[,paste0(j, "_", i)] <- 
                                                    evalue(paste0("data$", j, 
                                                                  "_", i)
                                                           ) /
                                                    evalue(paste0("data$EX_", i, "US")
                                                           )
                  }
}

clean()
  
##Explaination of the code above; a loop in a loop to paste the code the right way and convert all of the variables into USD values. This will facilitate the comparison and the times series analysis at a later step.
```

Transform Trade to cumulative trade variable
```{r}
country <- c("JP", "CH", "UK", "US")

for(j in country){
    total = 0
    for(i in 1:length(data$T_CH)){
      total = total + as.numeric(data[i, paste0("T_", j)])
      data[i, paste0("T_", j)] <- total
    }
    
}

clean()
```

Logarithm transformation
```{r}
series <- c("M1", "M3", "EX")
country <- c("JP", "CH", "UK", "US")
for(j in series)
{
  for(i in country)
  {
    if(j != "EX")
      {
      data[, paste0(j, "_", i)] <- log(data[, paste0(j, "_", i)])
      }
    else
      {
      if (i != "US")
        {
        data[, paste0(j, "_", i, "US")] <- log(data[, paste0(j, "_", i,"US")]) 
        }
      }
  }
}
  

clean()
```


###################################
Create independent variables series
###################################

To do that we have to create new series. Convert to data.frame to make this possible vectorizing.
```{r}
data2 <- as.data.frame(data)
```

The series to be created

(i) log(M1_US/M1_Foreign)

(ii) log(M3_US/M3_Foreign)

(iii) UN_US - UN_Foreign

(iv)  TB_US - TB_Foreign

(v)  CPI_US - CPI_Foreign
```{r}
## as the series are already in log. just take the difference
series <- c("TB", "M3", "UN", "M1", "CPI")
for(i in series){
  country <-  c("CH", "JP", "UK") 
  for(j in country){
    if((i == "TB") & (j == "UK")) next
    data2[, paste0(i, "_US",  j)] <- 
      data2[,paste0(i,"_US")] - data2[,paste0(i, "_", j)]
  }
} 
```

Convert back to xts
```{r}
data <- xts(data2, order.by=index(data))

## clean workspace
clean()
```


======================
Descriptive Statistics
======================

This section continues with general descriptive statistics trying to understand the behaviour of the exogenous series and trying to test whether the latter are indeed exogenous.

Keep just the series of interes
```{r}
data <- cbind(data[,27:ncol(data)], data[,1:3], data[,19:22])
```


Before diving into the descriptive analysis of the series, I will try to transform the series into stationary series so that it will possible to analyze the various series throught linear and non-linear times series models.

In order to transform the series into stationary series I will:

    1. Detrend the series.

    2. Remove the seasonal component.
    
    3. Test the series for unit roots.
    
    4. Perform chow-tests to check for structural breaks in the resulting series.


Write the data
```{r}
write(data, "rawdata.csv")
```


##################
Part 1: Detrending
##################

In a first step I will test the necessity for detrending the series.

Plot the series as a help:
```{r}
plotSeries <- 
              function(i)
              {
              plot(i, main = colnames(i))
              }

lis <- list()
lis <- lapply(data, plotSeries)
for(i in 1:ncol(data))
{
  if(i == 1) 
    par(mfrow = c(2,1))
  else if((i%%3 == 0) & !(i %in% c(18, 21)))
    par(mfrow = c(3,1))
  else if (i == ncol(data) -3)
    par(mfrow = c(2,2))
  plot(lis[[i]])
}
```


Plot proxy for variance as a help:
```{r}
varPlot <- function(i)plot((i- mean(i, na.rm = T))**2, main = colnames(i)) ## notice assumes time invariant mean, which is clearly violated in the series.

lis <- list()
lis <- lapply(data, varPlot)

for(i in 1:ncol(data))
{
  if(i == 1)
    par(mfrow = c(3,1))
  else if((i%%3 == 0) & !(i %in% c(18, 21)))
    par(mfrow = c(3,1))
  else if (i == ncol(data) -3)
    par(mfrow = c(2,2))
  plot(lis[[i]])                  
}
```

The majority of the series are clearly not mean stationary. 

For the exchange rate and the money supply it is difficult to judge based on simple plots, these might could appear to be mean stationary but not volatility stationary. 
```{r}
acf_plots <- function()
{
  for(i in 1:ncol(data)){
    if(i == 1)
      par(mfrow = c(2,1), mar = c(4,3,4,2))
    else if((i%%3 == 0) & !(i %in% c(18, 21)))
      par(mfrow = c(3,1), mar = c(4,3,4,2))
    else if (i == ncol(data) -3)
      par(mfrow = c(2,2), mar = c(4,3,4,2))
    acf(na.trim(data[,i]), main = colnames(data[,i]))                 
  }
}

## Plot acf
acf_plots() ## slow decaying autocorrelation points to non-stationarity mean series.
## The slow decay suggests that the series would be mean varyiant based on time shifts.
```


Step 2: Choose detrending solution

(i) Detrend via differenciation

(ii) Detrend via linear time trend

(iii) Use a moving average filter, this is a 'low pass' filter since it takes data and removes from it the rapidly fluctuating component  to leave the slowly varying estimated
trend.

Linear trend will provide to be ineffective for all of the times series.


Perform detrending via differentiation
```{r}
data2 <- data ## to play then with original dataset
data3 <- lapply(data, diff)
for(i in 1:ncol(data)){data[,i] <- data3[[i]]}
```


Plot new Variables
```{r}
lis <- list()
lis <- lapply(data, plotSeries)
for(i in 1:ncol(data)){
    if(i == 1)
      par(mfrow = c(2,1), mar = c(4,3,4,2))
    else if((i%%3 == 0) & !(i %in% c(18, 21)))
      par(mfrow = c(3,1), mar = c(4,3,4,2))
    else if (i == ncol(data) -3)
      par(mfrow = c(2,2), mar = c(4,3,4,2))
  plot(lis[[i]])
}
```

Plot proxy for variance for the new variables as a help:
```{r}
lis <- list()
lis <- lapply(data, varPlot)
for(i in 1:ncol(data))
{
  if(i == 1)
    par(mfrow = c(2,1), mar = c(4,3,4,2))
  else if((i%%3 == 0) & !(i %in% c(18, 21)))
    par(mfrow = c(3,1), mar = c(4,3,4,2))
  else if (i == ncol(data) -3)
    par(mfrow = c(2,2), mar = c(4,3,4,2))
  plot(lis[[i]])                  
}
```


Plot autocorrelation functions
```{r}
## Plot acf
acf_plots()
```


Result: Trade was a disaster. For the rest good improvement. Some adjustment still needed - see TB_US; possibly double differentiation. 

Compute moving average smoother for Trade.
```{r}
country <- c("JP", "CH", "UK", "US")

par(mfrow = c(2,2), mar = c(4,4,4,2))
for(i in country){
          trend <- ma(data2[, paste0("T_", i)], 
                      order = 4, centre = T) ##moving average of quartal
          data[, paste0("T_", i)] <- as.ts(data2[, paste0("T_", i)]) - trend
          acf(na.trim(data[,paste0("T_", i)]), main = paste0("T_", i)) 
         
}
```


Check for unit roots in the series performing augmented Dickey-Fuller tests
```{r}
suppressWarnings(lapply(data, adfTest)) ## supress warning to ignore message that true p-value smaller than the one printed
```


From the above two problematic series:

1. Treasury bills of US - JP

Solution: double differentiate the series
```{r}
data$TB_USJP <- diff(data$TB_USJP)
acf(na.trim(data$TB_USJP))
adf.test(na.trim(data$TB_USJP)) ## good
```

Will all of the above corrections the reslut is good and there is strong evidence for stationarity in the series.

Clean workspace and just keep the full dataset
```{r}
data <- na.trim(data)

## Update the clean function
clean <- function() 
{
  ENV <- globalenv()
  ll <- ls(envir = ENV)
  ll <- ll[!(ll %in% c("data", "clean", "import", "write", "evalue", "acf_plots",
                       "adfTest", "seasonality", "suspectFrequency", "spa"))]
  rm(list = ll, envir = ENV)
}

clean()
```


Step 3: analyze seasonality and correct for it.

Two possible solutions for adjusting for seasonality after identification of the cyclical frequency:

(i) Seasonal differentiation. Similar idea to the general differentiation of a series.

(ii) Subtract the block average of the series in the cycle. Similar idea to moving average smoothers.

Let's test now for the presence of seasonal patterns and let's try to identify the cyclical frequency. 

Checking again at autocorrelation plots
```{r}
acf_plots()
```


Well behaved series; adjust for annual seasonality by block differenciating.
```{r}
seasonality_12 <- function(x)
               {
                m_series = t(matrix(data = x, nrow = 12))
                season = colMeans(m_series, na.rm = T)
                data[,colnames(x)] <<- x - season
               }

## do an r test of the function tomorrow to check if the syntax is holds for the 13:24 and 25:36 observations.

invisible(suppressWarnings(lapply(data, seasonality_12))) 
## invisible does not return the lapply data assignments. The suppressWarnings tells us that for the last period there are not 12 observations and the demeaning is done with the remaining. This is because 190 is not a multiple of 12
```

Check at new adjusted
```{r}
acf_plots()
```

Perform test checking for seasonal unit root at monthly, quartarly frequency.
```{r}
lapply(data, seasonality, 4)
## good both quartely and monthly. 
## Notice the third argument is the ts-frequency argument specified in the function.R document
```

Perform FFT to check for possible other suspicious seasonality patterns for each series
```{r}
lapply(data, suspectFrequency)

## Trimestral frequency is worth to test.
```

Good. No other seasonal unit roots are displayed.

Clean workspace
```{r}
clean()
```


Before starting with the implementation of the linear modelling of the series I perform a chow test to understand the possibility of structural breaks in the sample that would suggest prudence when working with the series as a whole.

This is especially important as the period analyzed involves the outbreak of the dot-com bubble, which called for aggressive monetary policies and might significantly affect the results.
```{r}
prova <- function(x){
  
   ## I test against the alternative that each series can be fitted with a single constant     against the alternative of structural breaks that would call for multiple constants in      the sample.
  
  ## Using CUSUM
  ocus.x <- efp(as.ts(x) ~ 1, type = "OLS-CUSUM")
  
  #if(i == 1)
    #par(mfrow = c(2,1), mar = c(4,3,4,2))
  #else if((i%%3 == 0) & !(i %in% c(18, 21)))
    #par(mfrow = c(3,1), mar = c(4,3,4,2))
  #else if (i == ncol(data) -3)                ### search how to count over iterations of lapply
    #par(mfrow = c(2,2), mar = c(4,3,4,2))
  plot(ocus.x, main = colnames(x))
  
  ## Using dynamic programming of <cite> 
  ## https://eeecon.uibk.ac.at/~zeileis/papers/Zeileis+Kleiber+Kraemer-2003.pdf

  # store the breakdates
  #bp_ts <- breakpoints(x~ 1)

  # this will give you the break dates and their confidence intervals
  #summary(bp_ts) 
  
  #bp_ts$breakpoints

  # store the confidence intervals
  #ci_ts <- tryCatch(confint(bp_ts), error=function(e) NULL)
}

lapply(data, prova)

```


================================
Univariate times series analysis
================================

This section starts with the analysis of the cleaned dataset.

In a first step I will try to check whether the Meese-Rogoff paradox holds in the analyzed time period. I will work with the USA as the benchmark series and apply the different macroeconomics models to linearly estimate the FX_rates.

General model:

EX = const. + b1 M_USFR + b2 UN_USFR + b3 TB_USFR + b4 CPI_USFR + b5(TB_US) + b6 TB_FR

I will test the different models:

(i) Frenkel Bilson -> b4, b5, b6 = 0

(ii) Dornbusch-Frankel -> b5, b6 = 0

(iii) Hooper Morton -> no constraint


As in the case of Meese and Rogoff forecasting will be performed at one, three, six and twelwe months.

Cite Meese and Rogoff in the thesis: 
The purpose of considering multiple forecast horizons in this type of experiment is to see whether the structural models do better than time series models in the long run, when adjustment due to lags and/or a serially correlated error term has taken place. Of course, when lags and serial correlation are fully incorporated into the structural models, a consistently estimated true structural model will outpredict a time series model at all
horizons in a large sample.


## Notice in the section below you are estimating the models both with M1 and M3 --> adjust it.

===============
Random walk fit
===============

Random walk without drift prediction using rolling series
```{r}
## In the other models I will use 3/4 of the sample to train the data and the rest to predict. Be consistent to facilitate comaprison at a later stage.
three_fourth <- round(nrow(data)*(3/4))

## to create new series transform in data frame as before

data2 <- as.data.frame(data)

for(iPred in c(1,3,6,12)) 
  { 
    helper <- c("JP", "UK", "CH")
    for(jCountry in helper)
    {
    data2[, paste0("rwPred" , iPred, "_", jCountry, "US")] <- NA
    }
  } 

data2 <- as.xts(data2, order.by = index(data))## transform back to xts
data <- data2
rm(data2)
```

Fit the Random Walk prediction
```{r}
for(iPred in c(1,3,6,12)) 
  { 
    helpCountry <- c("JP", "UK", "CH")
    for(jCountry in helpCountry)
    {
        for(kRow in (three_fourth+1):nrow(data)) 
        {
         if ((iPred != 1) & (kRow+(iPred-2) == nrow(data))) break
         data[kRow+(iPred-1), paste0("rwPred" , iPred, "_", jCountry, "US")] <- 
           data[kRow-1, paste0("EX_", jCountry, "US")] 
        }
    }
  } 
```


==============================
Macroeconomic structural model
==============================

In contrast to Meese and Rogoff I decided to make use of the ARIMAX model here. This will allow the introduction of lagged exogenous variables and its implementation is favoured compared to the long lagged AR model used by Meese and Rogoff to my mind.

Specify rolling prediction
```{r}
## trasform to data-frame to add columns
data2 <- as.data.frame(data)

## create prediction columns

for(iPred in c(1,3,6,12)) 
  { 
    helper <- c("JP", "UK", "CH")
    for(jCountry in helper)
    {
    data2[, paste0("strPred" , iPred, "_", jCountry, "US")] <- NA
    }
  } 
```

Get exogenous variables for each of the series of interest
```{r}
## get exogenous parameters
for(country in c("CH", "UK", "JP"))
{
  assign(paste0("interest", country), 
         !str_detect(colnames(data2), paste0("^EX+\\D", country)) &
          str_detect(colnames(data2), paste0("\\D", country)) & 
         !str_detect(colnames(data2), "^rw|^str"))
}


## DO THIS NICE WITHOUT CODE REPETITION!!

## Add the trade balance sheet
interestUK[which(colnames(data2) == "T_US")] = T
interestCH[which(colnames(data2) == "T_US")] = T
interestJP[which(colnames(data2) == "T_US")] = T
```

### Arimax fit

```{r}
## transform to ts object ot leverage on the window function
data2 <- as.ts(data2, frequency =12)

## in sample observations
in_sample <- three_fourth;
  
## Get in sample time frame
st <- tsp(data2)[1] + (in_sample-2); 
## tsp(data2)[1] gets the start of the time frame; the second block adds the in_sample time window
```

Working well just last observation disappeared somehow.
```{r}
for( LAG in c(1, 3, 6, 12) ) 
{
  for( country in c("JP", "CH", "UK") )
  {  
    for( i in 1:(nrow(data2)-in_sample) )
    {
      xshort <- window(data2, start = tsp(data2)[1]+ (i-1), end=st + i)
      xnew <- window(data2, start = st + i, end = (st +i)+(LAG-1))
    
      ## Specify the arimax that fits at best
      nd_pars<-expand.grid(ar=0:2,diff=0:1,ma=0:2)
      nd_aic<-rep(0,nrow(nd_pars))
      help <- eval(parse(text = paste0("interest",country)))
      
      for(j in seq(along=nd_aic))
      {
        nd_aic[j]<-AIC(arimax(xshort[,paste0("EX_", country, "US")],
                              unlist(nd_pars[j,1:3]), 
                              xreg = xshort[, help],
                              method = 'ML'))
      }
    
      ## save the best fit
      order <- nd_pars[which.min(nd_aic),]
      
    
      fit1 <- arimax(xshort[, paste0("EX_", country, "US")], order = unlist(order), 
                     xreg = xshort[, help], method = 'ML')
      
      ## Create an exogenous vector and specify the structur in the case of lag == 1
      ## to keep the column structure in a consistent way to fit the model
      if ( LAG == 1) new_exogenous <- t(xnew[, help])
      else new_exogenous <- xnew[, help]
      
      fcast1 <- predict(fit1, n.ahead = LAG, newxreg = new_exogenous)
      if(index(tail(xshort,1)) == nrow(data2)-LAG) break 
      ## so that you have always 12 out of sample
      data2[in_sample+(LAG-1)+i, 
            paste0("strPred", LAG, "_", country, "US")] <- fcast1$pred[1]
    }
  }
}
```


===============================
Vector Autoregressive Modelling
===============================

```{r}
data2 <- as.data.frame(data2)
endogenous <- c("EX_UKUS", "EX_JPUS", "EX_CHUS")
exogenous <- !str_detect(colnames(data2), "^rw|^EX|^str")

data_exogenous <-  data2[1:three_fourth, exogenous]

VARselect(data2[1:three_fourth, endogenous], lag.max = 7,
          type = "none", exogen = data_exogenous) 
## one lag the best

model <- lineVar(data2[1:three_fourth, endogenous], 1, include = "none", 
                 model = "VAR", I = "level",
                 estim = "ML", LRinclude = "none", 
                 exogen = data_exogenous)

```

Create columns for inserting prediction results
```{r}
for(iPred in c(1,3,6,12)) 
  { 
    helper <- c("JP", "UK", "CH")
    for(jCountry in helper)
    {
    data2[, paste0("varPred" , iPred, "_", jCountry, "US")] <- NA
    }
  } 
```


Rolling prediction using VAR
```{r}
# number rolling prediction per forecasting time frame
for(i in c(1,3,6,12))
  {
  assign(paste0("numprediction", i), sum(!is.na(data2[, paste0("rwPred", i, "_JPUS")])))
  }

## Estimate the rolling window forecast
for(j in c("UK", "JP", "CH")) 
  {
    for(i in c(1,3,6,12))
    {
    parsed_text1 <- evalue(paste0("numprediction", i))
    assign(paste0("endRow",i), nrow(data2) +1 - parsed_text1)
    assign(paste0("pred_model", i), 
           predict_rolling(model, nroll = parsed_text1, n.ahead = i))
    parsed_text2 <- evalue(paste0("pred_model", i))
    parsed_text3 <- evalue(paste0("endRow", i))
    if(j == "UK") list_member = 1
    if(j == "JP") list_member = 2
    if(j == "CH") list_member = 3
    data2[parsed_text3:nrow(data2), 
         paste0("varPred", i, "_", j, "US")] <- parsed_text2$pred[list_member]
    }
}
```

Convert back to xts
```{r}
data2 <- as.xts(data2, order.by = index(data))## transform back to xts
data <- data2
```

TO DO: Would be nice to calculate the degrees of freedom in order to understand how reliable the model is.

Check at variance decomposition of the VAR to see whether the series present hgih endogeneity in the sense that the vector moving average model implied by the VAR modle suggests a high degree of spillovers from one variable to the next.

```{r}
plot(vars::fevd(model, ortho = T))
```

Endogeneity can be well ruled out in the series for the UKUS dollar and JPUS foreign exchange rates. This does not hold strictly for the CHUS foreign exchange rate that seems to be well infulenced by the UKUS forex evolution.

Clean working space
```{r}
clean()
```

Save results into a csv
```{r}
write(data, "linear.csv")
```



============================
Autocorrelation of the Error
============================

Check at autocorrelation of error series to see whethere the residuals of the models are white noise of whether there is systematic information that was not captured.
```{r}
# number rolling prediction per forecasting time frame
for(i in c(1,3,6,12))
  {
  assign(paste0("numprediction", i), sum(!is.na(data[, paste0("rwPred", i, "_JPUS")])))
  }

# Plot acf and test for white noise
for ( iLag in c(1, 3, 6, 12) )
{
  for( country in c("CH", "JP", "UK") )
  {
    par(mfrow = c(3,1))
    for( method in c("rw", "str", "var") )
    {
      error <- tail(data[, paste0("EX_", country, "US")],
                          evalue(paste0("numprediction", iLag))
                          ) - 
                na.trim(data[, paste0(method, 
                                    "Pred", iLag,"_", 
                                    country, "US")
                            ]
                        )
      acf(error, main = paste("Pred", method, iLag, country))
      
      ## Back up with quantitative Ljung Box measure
      for( jLag in c(5,20,50) )
      {
      print(paste("Pred", method, iLag, country, "Ljung", jLag))
      print(Box.test(error, lag = jLag, type = "Ljung-Box"))
      }
      
    }
  }
}
```


======================================================
Out of sample performance comparison for linear models
======================================================

In this section I will compare the performance of the different linear models in the out of sample predicition fit.

Plot the different out of sample forecasting fits
```{r}
plot(tail(data$EX_CHUS, 50), ylab="Rates", main="Linear Estimators - 1 Lag Out of Sample", 
     grid.col = NA, col = 1)
lines(data$rwPred1_CHUS,col= 2, lwd = 2)
lines(data$strPred1_CHUS, col = "orange", lwd = 2)
lines(data$varPred1_CHUS, col = "yellow", lwd = 2)
addLegend("topleft", on=1, 
          legend.names = c("Original", "RW", "ARIMAX", "VAR"), 
          lty=c(1, 1), lwd=c(2, 1),
          col=c(1, 2, "orange", "yellow"))
```


* Step 2 - Quantitative Measure for Prediction Power

Root Mean Squared Error (MSE)
```{r}
rmseFun <- 
          function(error)
          {
            #error <- Actual - Forecast
            #print(error)
            return( sqrt(sum(error**2)/length(error)) )
          }
```

Mean Absolute Error (MAE)
```{r}
maeFun <- 
            function(error)
            {
              #return( Actual - Forecast )
              return( sum(abs(error))/length(error) )
            }
```

Directional Accuracy (DAC)
```{r}
dacFun <- 
          function(Actual, Forecast, LAG)
          {
            return( mean ( sign( diff(Actual, lag=LAG) ) == 
                           sign( diff(Forecast, lag=LAG) ),
                           na.rm = T
                          ) 
                  )
          }
```


Create Series of Forcasting Errors; repeat for all of the lags.
```{r}

# number rolling prediction per forecasting time frame
for(iLag in c(1,3,6,12))
{
assign(paste0("numprediction", iLag), sum(
                                          !is.na(
                                                  data[, 
                                                      paste0("rwPred", iLag, "_JPUS")
                                                      ]
                                                 )
                                          )
       )
  
}

RMSE <- matrix(NA,3,3, 
               dimnames = list( c("rw", "str", "var"),
               c("CH", "JP", "UK") )
               )

MAE <- matrix(NA,3,3, 
               dimnames = list( c("rw", "str", "var"),
               c("CH", "JP", "UK") )
               )

DAC <- matrix(NA,3,3, 
               dimnames = list( c("rw", "str", "var"),
               c("CH", "JP", "UK") )
               )
```


Compute root mean square error and mean absolute error for the different lags and linear estimators
```{r}
errorFun <- 
            function(LAG)
            {
              iRow = 0
              jCol = 1
              for( country in c("CH", "JP", "UK") )
              {
                for( method in c("rw", "str", "var") )
                {
                  iRow = iRow + 1
                  
                  actual_data <- tail(data[, paste0("EX_", country, "US")],
                                      evalue(paste0("numprediction", LAG))
                                      )
                  
                  forecast_data <- na.trim(data[, paste0(method, 
                                                "Pred", LAG,"_", 
                                                country, "US")
                                               ]
                                           )
                  
                  error_data <- actual_data - forecast_data
                  
                  RMSE[iRow, jCol] <- rmseFun(error_data)
                  
                  MAE[iRow, jCol] <- maeFun(error_data)
                  
                  DAC[iRow, jCol] <- dacFun(actual_data, forecast_data, LAG)
                  
                  if(iRow %% 3 == 0)
                  {
                    jCol = jCol + 1
                    iRow = 0
                  }
                
                }
              }
              
              print(paste("RMSE", LAG))
              print(RMSE)
              
              print(paste("MAE", LAG))    
              print(MAE)
              
              print(paste("DAC", LAG))    
              print(DAC)
            }
```

Evalue the function for different lags
```{r}
for( iLAG in c(1, 3, 6, 12))
errorFun(iLAG)
```

============
Augmented MZ 
============

Test if the model can be restricted; i.e. test the null hypothesis of the different coefficients.

!!!! USE AUGMENTED MZ TESTS TO CHECK WHETHER MORE SIMPLE MODELS HOLD IN THE DATA.

====================
Model Confidence set
====================

Model Confidence set
```{r}
#Create u a matrix with all different univariate GARCH predictions: each column, one model prediction
u <- na.trim(cbind(data$rwPred1_JPUS, data$varPred1_JPUS, data$strPred1_JPUS))

n<-dim(u)[1]
perf<-matrix(NA,n,3)

### Adjust length of perf for dac if you want compute it.

for (j in 1:3)
{
  #perf[,j]<- na.trim(
  #                    sign( diff(tail(data$EX_JPUS[-nrow(data)], n), 
  #                               lag=1) 
  #                          ) == 
  #                         sign( diff(u[,j], lag=1) )
  #                   ) # DAC
  perf[,j]<- abs(tail(data$EX_JPUS[-nrow(data)], n)-u[,j]) #MAE
  #perf[,j] <- abs(tail(data$EX_JPUS[-nrow(data)], n)-u[,j])^2 #MSE
}

#print(colMeans(perf, na.rm = T))
## Perf is a matrix containing for each column the loss function per observation of one forecasting method. 
## Taking the colMeans corresponds 1:1 to the tables above; notice however that here you work with Mean Square Error and NOT Root Mean Square Error.
  
for (k in 1:3)
{
  print(k)	
  d<-rep(NA,1000)
  s<-1
  for (i in 1:10) ## make 10 times 100 boots = 1000 the number of d specified above
  { 
  	#print(i)
     e<-spa(per=perf,column_benchmark_model=k,
            num_predmodels=3,number_predictions=n,q=0.25,iter=100,periodogram=T)
     d[s:(s+99)]<-e$stat.boos
     s<-s+100
  }
  print(mean((d>e$stat))) ## fraction 
}

## With MSE
## -> random walk excluded wit 90% confidence

## With MAE
## -> random walk excluded with 99% confidence
## -> arimax with 

```

==========================
Generalized Additive Model
==========================
