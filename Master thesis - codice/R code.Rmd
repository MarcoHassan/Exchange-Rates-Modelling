---
title: "Master thesis code - FX rates. A nonlinear evaluation"
output:  rmarkdown::github_document
---

Libraries used
```{r}
library(quantmod) ##same principle as Quandl, not limited by the amount of API calls though and connect to multiple daset providers. In this study we will mainly query the FRED database and extract data by referring to their API.
library(forecast) ## for seasonality adjusting the times series
library(tseries) ## -> check when used
library(TSA) ## for applying FFT to get seasonality frequencies
```

===============
Data Collection
===============

Meese and Rogoff also used the Forward rates for their estimation.

Specify function to get symbols
```{r}
get_Symbols <- function(x){
                           lapply(x, function(sym){getSymbols(sym, src = "FRED",
                                                              return.class = "xts",
                                                              auto.assign = F)})
  ## auto.assign specifies that not the ticker should be passed
}
```

###########
FX - rates
###########

Download foreign exchange rates
```{r}
Symbols <- c("EXUSUK", "EXJPUS", "EXSZUS")

FX <-  get_Symbols(Symbols)

## Invert the USUK rate to UKUS to keep it consistent with the other series
FX[[1]] <- 1/FX[[1]]

FX <- do.call(merge, FX)

colnames(FX)[] <- c("EX_UKUS", "EX_JPUS", "EX_CHUS")

## notice like that all of the evaluation will be performed compared to the usa. if you want to make smaller bilateral comparison you need to download the other forex.
```

###
GDP
###

GDP is an aggregated statistic that is published on a quarterly basis. You can use monthly proxies. Electricity consumption? Industrial production? <-> this is normalized to 100. Is that good?

Unemployment not good. Does not capture the size of an economy. Here it will be important.
```{r}
## use industrial production? 
## use unemployment rate. This approach preferred
Symbols <- c("LRUN64TTUSM156N", "LRUN64TTJPM156N", "LMUNRRTTCHM156N", "LMUNRRTTGBM156N") #F not seasonally adjusted.

UNply <- get_Symbols(Symbols)

UNply <- do.call(merge, UNply) ## notice the unemployment series for UK starts just in the 90s.

colnames(UNply) <- c("UN_US", "UN_JP", "UN_CH", "UN_UK")
```


################
Short term bills
################

Treasury bills for capturing short term attractiveness of investments
```{r}
Symbols <- c("TB3MS","INTGSTJPM193N","TBDRUKM")
 ## 3 months bill rate usa; secondary market.
 ## bill for japan; explaination not clear. check at it again to clearly understand what it is precisely.
 ## treasury bill for uk

bills <- get_Symbols(Symbols)

bills<- do.call(merge, bills) 

colnames(bills) <- c("TB_US", "TB_JP", "TB_CH") ## Treasury bills

## above not available for switzerland. use interbank rate for it.
```


#############
Monetary base
#############

Problem you are taking differentials for all of the above measures. The values should consequently be consistent and comparable. Use M3 or M1 for all; these should be similarly defined.
```{r}
Symbols <- c("MYAGM3USM052N", "MYAGM3JPM189N", "MABMM301GBM189N", "MABMM301CHM189N") #us, jp, uk, not seasonally adjusted. 

M3 <- get_Symbols(Symbols)

M3 <- do.call(merge, M3)

colnames(M3) <- c("M3_US", "M3_JP", "M3_UK", "M3_CH")

## all the values in national currency. Will need to transform them at a later period using FX-rate and then take the logarithm.

```

Extract M1
```{r}
Symbols <- c("M1NS", "MANMM101JPM189N", "MANMM101GBM189N", "MANMM101CHM189N") 

M1 <- get_Symbols(Symbols)

M1 <- do.call(merge, M1)

colnames(M1) <- c("M1_US", "M1_JP", "M1_UK", "M1_CH")


## all the values in national currency. Will need to transform them at a later period using FX-rate and then take the logarithm.
```

###
CPI
###

```{r}
## all not seasonally adjusted, all with basis 2015
Symbols <- c('CHECPIALLMINMEI', 'JPNCPIALLMINMEI', 'GBRCPIALLMINMEI', 'USACPIALLMINMEI')

CPI <- get_Symbols(Symbols)

CPI <- do.call(merge, CPI)

colnames(CPI) <- c("CPI_CH", "CPI_JP", "CPI_UK", "CPI_US")
  
## Important difference, lapply applies a function over a list, do.call calls a function with a list of arguments.

## CHECPIALLMINMEI ## switzerland from FRED; harmonised index; basis 2015
## JPNCPIALLMINMEI ## japan; basis 2015
## GBRCPIALLMINMEI ## uk; basis 2015
## USACPIALLMINMEI ## usa, basis 2015

## good all with the same basis
```

##############
Trade balances
##############

```{r}
## Current accounts are available just on quartal level. Two options use current accounts and hold it fix for the entire quartal.

## Option two; chosen option, find a proxy for the current account balance.

Symbols <- c("XTNTVA01CHM664N", "XTNTVA01JPM664N", "XTNTVA01USM664N", "XTNTVA01GBM664N")
 ## net trade. monthly not seasonally adjusted. broad proxy for current account balance.

Trade <- get_Symbols(Symbols)

Trade <- do.call(merge, Trade)

colnames(Trade) <- c("T_CH", "T_JP", "T_US", "T_UK") 

## check at the currency. All in national currency! Convert them. 
```


###########################
Merge all of the statistics
###########################

```{r}
data <- merge.xts(FX, bills, CPI, M1, M3, Trade, UNply) ##dagli anni '90 al 2005 hai tutte le statistiche.

rm(list=setdiff(ls(), "data")) ## clean the workspace just keeping the data

data <- data["1990-01-01/2006-02-01"] ## extract a complete dataset without NA; notice if you run the analysis with M1 you have complete data until 2013.

length(data[,1]) ## 194 observations per ts

## Transform all the variables in USD currency
```

UK statistics are poor. For the other countries you can run the analysis back to the '70s. For UK because of M3 and especially unemployment rate the comparison is much more limited.

###################
Data transformation
###################

Transform M1, M3 and Trade balance sheets in USD. Compute cumulative Trade balance sheet.
Transform also the interest rate in USD interest rate. Assume FX-rate martingale. Otherwise you would have to download the forward interest rate. 
```{r}
helper1 <- c("M1", "M3", "TB", "T")
for(j in helper1){
                  helper2 <- c("JP", "CH", "UK")
                  help <- function(x){eval(parse(text = x))}
                  for (i in helper2){
                       if(paste0(j, "_", i) != "TB_UK")
                       data[,paste0(j, "_", i)] <-help(paste0("data$", j, "_", i))/help(
                                                              paste0("data$EX_", i, "US"))
                  }
}
rm(helper1)
rm(list = setdiff(ls(), "data"))
  
##Explaination of the code above; a loop in a loop to paste the code the right way and convert all of the variables into USD values. This will facilitate the comparison and the times series analysis at a later step.
```

Transform Trade to cumulative trade variable
```{r}
helper2 <- c("JP", "CH", "UK", "US")
for(j in helper2){
    total = 0
    for(i in 1:length(data$T_CH)){
      total = total + as.numeric(data[i, paste0("T_", j)])
      data[i, paste0("T_", j)] <- total
    }
    
}
rm(list = setdiff(ls(), "data"))
```

Logarithm transformation
```{r}
transformation <- c("M1", "M3", "EX")
helper2 <- c("JP", "CH", "UK", "US")
for(j in transformation){
    for(i in helper2){
      if(j != "EX"){
         data[, paste0(j, "_", i)] <- log(data[, paste0(j, "_", i)])
      }
      else{if(i != "US") data[, paste0(j, "_", i, "US")] <- log(data[, paste0(j, "_", i,
                                                                              "US")])}
    }
    
}
rm(list = setdiff(ls(), "data"))
```

======================
Descriptive Statistics
======================

This section continues with general descriptive statistics trying to understand the behaviour of the exogenous series and trying to test whether the latter are indeed exogenous.

Before diving into the descriptive analysis of the series, I will try to transform the series into stationary series so that it will possible to analyze the various series throught linear and non-linear times series models.

In order to transform the series into stationary series I will:

    1. Detrend the series.

    2. Remove the seasonal component.
    
    3. Test the series for unit roots.
    
    4. Perform chow-tests to check for structural breaks in the resulting series.
    
    
    
##################
Part 1: Detrending
##################

In a first step I will test the necessity for detrending the series.

Plot the series as a help:
```{r}
help <- function(i){
  plot(i, main = colnames(i))
}
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if((i == 1) || (i == 4))
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])
}
```


Plot proxy for variance as a help:
```{r}
help <- function(i)plot((i- mean(i))**2, main = colnames(i)) ## notice assumes time invariant mean, which is clearly violated in the series.
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if(i == 1)
    par(mfrow = c(3,1))
  if(i == 4)
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])                  
}
```

All the series are clearly not mean stationary. 

Just for the exchange rate it is difficult to judge based on simple plots, these might could appear to be mean stationary but not volatility stationary. 
```{r}
help <- cbind(data$EX_UKUS, data$EX_JPUS, data$EX_CHUS)

par(mfrow = c(2,2))
lapply(help, acf) ## slow decaying autocorrelation points to non-stationarity mean series.
## The slow decay suggests that the series would be mean varyiant based on time shifts.
```


Step 2: Choose detrending solution

(i) Detrend via differenciation

(ii) Detrend via linear time trend

(iii) Use a moving average filter, this is a 'low pass' filter since it takes data and removes from it the rapidly fluctuating component  to leave the slowly varying estimated
trend.

Linear trend will provide to be ineffective for all of the times series.


Perform detrending via differentiation
```{r}
data2 <- data ## to play then with original dataset
data3 <- lapply(data, diff)
for(i in 1:ncol(data)){data[,i] <- data3[[i]]}
```


Plot new Variables
```{r}
help <- function(i){
  plot(i, main = colnames(i))
}
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if((i == 1) || (i == 4))
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])
}
```

Plot proxy for variance for the new variables as a help:
```{r}
help <- function(i)plot((i- mean(i, na.rm = T))**2, main = colnames(i)) ## notice assumes time invariant mean, which is clearly violated in the series.
lis <- list()
lis <- lapply(data, help)
for(i in 1:ncol(data)){
  if(i == 1)
    par(mfrow = c(3,1))
  if(i == 4)
    par(mfrow = c(3,1))
  else if (((i+1)%%4 == 0) & (i != 3))
    par(mfrow = c(2,2))
  plot(lis[[i]])                  
}
```


Plot autocorrelation functions
```{r}
acf_plots <- function(){
  for(i in 1:ncol(data)){
    if(i == 1)
      par(mfrow = c(3,1), mar = c(4,3,4,2))
    if(i == 4)
      par(mfrow = c(3,1), mar = c(4,3,4,2))
    else if (((i+1)%%4 == 0) & (i != 3))
      par(mfrow = c(2,2), mar = c(4,3,4,2))
    acf(na.trim(data[,i]), main = colnames(data[,i]))                 
  }
}

## Plot acf
acf_plots()
```


Result: Trade was a disaster. For the rest good improvement. Some adjustment still needed - see TB_US; possibly double differentiation. 

Compute moving average smoother for Trade.
```{r}
helper1 <- c("JP", "CH", "UK", "US")

par(mfrow = c(2,2), mar = c(4,4,4,2))
for(i in helper1){
          trend <- ma(data2[, paste0("T_", i)], 
                                       order = 4, centre = T) ##moving average of quartal
          data[, paste0("T_", i)] <- as.ts(data2[, paste0("T_", i)]) - trend
          acf(na.trim(data[,paste0("T_", i)]), main = paste0("T_", i)) 
         
}
```


Check for unit roots in the series performing augmented Dickey-Fuller tests
```{r}
help <- function(x){
        dd <- adf.test(na.trim(x))
        if(dd$p.value >= 0.05) dd
}

suppressWarnings(lapply(data, help)) ## supress warning to ignore message that true p-value smaller than the one printed
```


From the above two problematic series:

1. Treasury bills of US.

2. Unemployment rate of UK.

Solution: double differentiate the both
```{r}
data$TB_US <- diff(data$TB_US)
data$UN_UK <- diff(data$UN_UK)
acf(na.trim(data$TB_US))
acf(na.trim(data$UN_UK))

```

Will all of the above corrections the reslut is good and there is strong evidence for stationarity in the series.

Clean workspace and just keep the full dataset
```{r}
data <- na.trim(data)
rm(list=setdiff(ls(), "data"))
```


Step 3: analyze seasonality and correct for it.

Two possible solutions for adjusting for seasonality after identification of the cyclical frequency:

(i) Seasonal differentiation. Similar idea to the general differentiation of a series.

(ii) Subtract the block average of the series in the cycle. Similar idea to moving average smoothers.

Let's test now for the presence of seasonal patterns and let's try to identify the cyclical frequency. 

Checking again at autocorrelation plots
```{r}
acf_plots()
```

Evidence for strong autocorrelation at yearly lag in the series. Make a quantiative test
```{r}
seasonality_12 <- function(x){
               ## test for unit root for given seasonality
               dat <- ts(x, frequency = 12)
               
               # Test for the presence of seasonal patterns at the determined frequency
               res <- nsdiffs(dat)
               
               # If the test successful indicating unit root differenciate 
               if(res == 1) {
                 #data[-1:-12,colnames(x)] <- diff(dat, lag = 12)
                 decomposed <- decompose(dat, "additive")
                 data[,colnames(x)] <<- dat - decomposed$seasonal
                 ## understand online how this decompose function performs seasonal adjustments --> no observation lost!! --> weird
               }
}

lapply(data, seasonality_12)
```

Check at new adjusted
```{r}
acf_plots()
```

Perform FFT to check for possible other seasonal patterns
```{r}
#data <- data2
seasonality <- function(x){
               p = periodogram(x, plot = F) ## short lag determinant
               dd = data.frame(freq=p$freq, spec=p$spec)
               order = dd[order(-dd$spec),]
               top = head(order, 1)
               
               # convert frequency to time periods
               time = round(1/top$f)
               
               ## test for unit root for given seasonality
               dat <- ts(x, frequency = time[1])
               
               # Test for the presence of seasonal patterns at the determined frequency
               res <- nsdiffs(dat)
               
               if(res == 1) {
                 warning( "Seasonality at FFT frequency" )
               }
}

lapply(data, seasonality)
```


Clean workspace
```{r}
rm(list = setdiff(ls(), c("acf_plots", "data")))
```


================================
Univariate times series analysis
================================

White noise prediction
```{r}

```




