\section{Methodology}
\label{sec:part2}

\subsection{Structural Model}

The basic structural model encompassing all of the different monetary models
discussed in the introductory session is of the following form:

\begin{equation} \label{eq:structural}
  s = \beta_{0} + \beta_{1}(m - m^{*}) + \beta_{2}(y-y^{*}) + \beta_{3}(r-r^{*})
  + \beta_{4}(\pi - \pi^{*}) + \beta_{5}(TB - TB^{*}) + \varepsilon
\end{equation}

where $s$ represents the logarithm of the indirect quote of FX-rates, i.e. the
foreign exchange value of a dollar unit, $m-m^{*}$ represents the logarithm of
the U.S. and foreign country money mass supply ratio, $y-y^{*}$ represents the logarithm
of the U.S. and foreign GDP level ratio, $r-r^{*}$ represents the short term U.S./foreign country
interest rate differential, $\pi - \pi^{*}$ represents the U.S./foreign country inflation
rate differential, $TB - TB^{*}$ represents the U.S. foreign country current account differential.
Finally, $\varepsilon$ represents the zero expectation error term capturing all of the
other factors not expressed in the model.\\
The model above encompasses all of the monetary models previously discussed. Specifically, the most basic monetary model, the Frenkel-Bilson model logically inferred from
the relative PPP proposition, assumes $\beta_{4} = \beta_{5} = 0$. The Dornbusch model that allows for
a sluggish price adjustment behaviour and predicts FX-overshooting
assumes $\beta_{5} = 0$. Finally the Hooper-Morton model poses no restrictions on
the coefficients of equation \ref{eq:structural}. Such a structural model is consequently
estimated on four different parametric models of interest, which will be discussed next.

\subsection{Univariate Models}
\label{sub:univariate}

The most basic model that is tested is an OLS model as in the benchmark papers of Meese
and Rogoff (1983a, 1983b, \cite{MeeseRogoffa, MeeseRogoffb}). In this model the
coefficients of equation \ref{eq:structural} are computed without taking into account any lagged effect.
On the other hand, the second more flexible model is modeled to capture possible lagged effects
of the macroeconomics fundamentals. Opposed to the Meese and Rogoff papers where the authors decided
to capture the possibility of lagged terms by incorporating autoregressive models giving a higher
importance on more recent observations we decided to apply the transfer function models widely spread in
the fields of engineering such as control systems and electronic circuits. These models have been poorly
discussed in the field of economics with the exception of Tustin (1957, \cite{Tustin}) that tried to
make the point for applying the models in the economic modeling field.\\
Transfer function models relates a given set of inputs to an output variable through the following general formula

\begin{equation} \label{eq:transfer}
  Y_{t} = \mu + \frac{(\omega_{0}+\omega_{1}B^{1}+\dots+\omega_{s}B^{s})}{1-\delta_{1}B^{1}-\dots-\delta_{r}B^{r}}X_{t-b} + \varepsilon_{t}
\end{equation}

where $X$ represent a matrix of exogenous terms, $\mu$ the optional term modeling the mean of the series and
$\varepsilon$ the non-captured variation in the series. Moreover the above ratio represents the transform
function of the regressors matrix and is especially characterized by the order of the denominator and
nominator terms. \\
$r$, the order of the denominator term, expresses the rate of the decay pattern, where
a higher term indicates a slower decay.  $s$, the order of the nominator term, expresses the persistence of
so called unpatterned spikes, that is the persistence of effects that are not captured in the decay pattern.
Finally, the $b$ term in the matrix of input represents the dead time, that is the time it takes for the dependent
variable to react to some changes in the input matrix. This is of primary importance as it might very well be
that some shift in macroeconomic fundamentals just begins to display effects after a certain amount of time when
the economic agents begin to perceive the change.\\
Due to the flexible nature of the latter model, we believe that it is better suited and preferable to
capture the true nature of distributed lags present in the structural model of equation \ref{eq:structural} in
comparison with the smoothed autoregressive model, applied by Meese and Rogoff, with the arbitrarily chosen smoothing
term of 0.95 (See Meese and Rogoff (1983a pp. 7, \cite{MeeseRogoffa})).


\subsection{Multivariate Models}
\label{sub:multivariate}
The above discussed models rely on the exogeneity of the independent variables. If the assumption
fails, the model are biased and lead to misleading conclusion as the independence of the
sample distribution is not guaranteed. In practice this poses an issue for the estimation
of the structural model of equation \ref{eq:structural}.

While some variables as the monetary
mass and the output gap are commonly treated as exogenous variables in the underlying monetary models
the practice suggests that such macroeconomic variables might very well be influenced by the
movements of FX-rates. On the top of it other variables such as short-term interest rates differentials
are treated as endogenous even in the underlying monetary models and therefore require a different estimation
compared to the one outlined in the previous section.

To obviate the above issues Meese and Rogoff (1983a and 1983b, \cite{MeeseRogoffa, MeeseRogoffb}) analyzed the structural model
of interest through a vector autoregressive model firstly introduced by Sims (1980, \cite{Sims1980}).
In the general case the model consists of a system of equations in the form

\begin{equation} \label{eq:VAR}
y_{t} =  \Phi_{0}+ \Phi_{1}y_{t-1} + \Phi_{2}y_{t-2} + \dots + \Phi_{n}y_{t-n} + \varepsilon_{t}
\end{equation}

where $y$ is a vector containing the endogenous variables of interest, $\Phi_0$ is a vector of constants,
$\Phi_1,  \dots  , \Phi_n$ are matrices describing the effect of lagged endogenous variables on the levels of
the current variables and $\varepsilon$ captures the equations specific error term.
The resulting model will capture the endogeneity present in the monetary structural model allowing for a consistent
OLS estimation as far as the error terms in the equations are uncorrelated.

Despite the described vector autoregressive model manages well to model the endogeneity of macroeconomic
variables we question whether the restricted form of it could yield more efficient and more reliable estimates capturing
the FX-rates and the monetary models relation. This is especially motivated by the cointegration theory developed
by Engle and Granger (1987, \cite{EngleGranger}) and the well known evidence of non stationary macroeconomics
times series \footnote{See for instance Gil-Alana and Robinson (1997, \cite{GilAlanaRobinson})}. Moreover, the importance
of such method is underlined by the evidence of Phillips (1986, \cite{Phillips}) that set down the theoretical fundamentals
showing how parameter estimates of cointegrated series do not converge in probability and do not converge to any
non-degenerate distribution in the asymptotic case if the case of a misspecified OLS estimate as potentially is equation \ref{eq:VAR}.

We propose therefore a test for cointegration among the macroeconomic and the FX-rates series
based on Johansen (1991, \cite{Johansen}) %% scrivere Johansen procedure ??%%
and we consequently estimate a vector error correction model of the form

\begin{align}
  \Delta y_t =& \ \Pi y_{t-1} + \sum^{p-1}_{i=1} \Phi^{*} \Delta y_{t-i} + \varepsilon_t \nonumber\\
  \Phi^{*} =& \ - \sum^{p}_{i=j+1} \Phi_i, \ \  j = 1, \dots, p-1 \label{eq:VECM}\\
  \Pi =& \ -(I - \Phi_1 - \dots - \Phi_p) \nonumber\
\end{align}

where $\Pi y_{t-1}$ of equation \ref{eq:VECM} represent the error correction term and $y_i$ and $\Phi$
refer to the variables described in the unrestricted vector autoregressive model of \ref{eq:VAR}.

\subsection{Forecasting Approach}
\label{sub:forecast}
The four described models and a random walk model without drift are validated by looking
at their ability to forecast FX-rates out of sample. In this sense the 234 observations sample
is split in a training and a validation sample. Three fourth of the total observations are
training the four models while the remaining fourth of the observations is used for the
out of sample validation.

In comparison to Meese and Rogoff
(1983b, \cite{MeeseRogoffb}) we do not try any restricted estimation based on the theoretical
monetary models literature. We rather estimate unrestricted versions for all of the models
outlined with the exception of the vector error correction model described in \ref{eq:VECM}, given
the by product VAR model restriction imposed by the latter.

To select the best performing monetary model we use a two step approach. Firstly, we estimate the four 
parametric models according to the most general structural model
described in \ref{eq:structural}. Secondly we perform Wald tests to check whether there is in sample
evidence for using restricted models. Based on positive evidence we proceed by running restricted models
out of sample and look for an increased performance.

The out of sample performance of the different models is measured by applying a rolling
out-of-sample forecast in analogy to the benchmark papers. This consists 
of a re-estimation of the four outlined models for each new forecasting point. Given the decision
to estimate the out of sample model performance at one, three, six and twelve months lags the rolling
forecast technique involves a model reestimation at respective frequencies. Important is to underline how
the results of the Wald tests are extended for each of the subsequent model estimation
in the rolling forecast. This means that given the statistically significant evidence for the Null of a
restricted model in the first three fourth of the sample the same restricted model is used for the
subsequent model estimations. 

Explicitly, our approach intends to fit a structural model containing all of the macroeconomics
series in the first 176 observations - from September 1986 to April 2001 - and perform Wald
tests to verify whether a restricted monetary model is supported in the sample. Based on such restults
the rolling forecast method intends to estimate the next point forecast at 1, 3, 6 and 12 month lags
and shift the data sample of one period such that a new model is estimated for
the October 1986 to May 2001 series. According to the new parametric fit,
point forecast at 1, 3, 6 and 12 months are estimated and the method iterated until the last out of sample
observation for February 2006 is reached.

Finally, it is important to underline that as in the benchmark papers, we allow the univariate models
described in \ref{sub:univariate} a richer set of information compared to the random walk.
Specifically, the random walk and the multivariate model use the $\mathscr{F}_{t-1} = {F_0, \ \dots, \ F_{t-1}}$
information set, where $F_i$ represents a set containing all of the available information at timepoint $i$.
By contrast the univariate models dispose of $\mathscr{F}_t \setminus s_t$, where $s_t$ represents
the FX-rate at time point $t$.
In simple terms this means that we are going to give the univariate models the advantage of using the actual realizations of
macroeconomics variables without the need to estimate them, therefore outlying the possibility of poor out-of-sample fit
due to poor macroeconomic fundamentals estimation. 

Based on the obtained rolling forecasts three statistics are computed to compare the model fit.
These are the root mean squared error (RMSE), the mean absolute error (MAE) and the mean directional accuracy (MDA).
A particular importance is assigned to the MAE results given Westerfield (1977, \cite{Westerfield})
that analyzed the empirical exchange rates distribution finding evidence for FX-rates
non-normal stable-Paretian distributions with infinite variance.\\
Opposed to Meese and Rogoff (1983a and 1983b, \cite{MeeseRogoffa, MeeseRogoffb}) we decided
to further explore the point estimators of the above mentioned statistics by computing a model confidence set (MCS)
as described in Hansen et al. (2011, \cite{HansenMCS}).
The idea of the latter consists of a procedure yielding a model set, $\mathcal{M}^*$, built to contain the
best model with a chosen level of confidence. The exact procedure is based on an equivalence test $\delta_m$ and an
elimination rule $\epsilon_m$, consistent with the chosen confidence level. In a first step the competing models are
compared with each other. If $\delta_m$ does not support evidence for the equal performance of the models, $\epsilon_m$
is applied, the poorly performing models are discarded, and the general problem reiterated until $\delta_m$ is accepted
for all of the surviving models.\\
In this paper we decided to make use of superior predictive ability test of Hansen (2005, \cite{HansenSPA})
and to obtain p-values for the equal predictive hypothesis of models according to a bootstrap implementation
outlined by Hansen et al. (2011, \cite{HansenMCS}).