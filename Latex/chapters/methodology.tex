\section{Methodology}
\label{sec:part2}

\subsection{Structural Model}
\label{sub:structural}

The basic structural model encompassing all of the different monetary
models discussed in the introductory session is of the following form:

\begin{equation} \label{eq:structural}
  s = \beta_{0} + \beta_{1}(m - m^{*}) + \beta_{2}(y-y^{*}) + \beta_{3}(r-r^{*})
  + \beta_{4}(\pi - \pi^{*}) + \beta_{5}(TB - TB^{*}) + \varepsilon
\end{equation}

where $s$ represents the logarithm of the indirect quote of FX-rates,
i.e. the foreign exchange value of a dollar unit, $m-m^{*}$ represents
the logarithm of the U.S. and foreign country monetary mass supply ratio,
$y-y^{*}$ represents the logarithm of the U.S. and foreign GDP level
ratio, $r-r^{*}$ represents the short term U.S./foreign country
interest rate differential, $\pi - \pi^{*}$ represents the
U.S./foreign country inflation rate differential, $TB - TB^{*}$
represents the U.S. foreign country current account differential.
Finally, $\varepsilon$ represents the zero expectation i.i.d. error
term capturing all of the
other factors not expressed in the model.\\
The model above encompasses all of the monetary models previously
discussed. Specifically, the most basic monetary model, the
Frenkel-Bilson model logically inferred from the relative PPP
proposition, assumes $\beta_{4} = \beta_{5} = 0$. The Dornbusch model
that allows for a sluggish price adjustment behaviour and predicts
FX-overshooting assumes $\beta_{5} = 0$. Finally the Hooper-Morton
model poses no restrictions on the coefficients of equation
\ref{eq:structural}. Such a structural model is consequently estimated
on five different parametric models of interest, which will be
discussed next.

\subsection{Univariate Models}
\label{sub:univariate}

The most basic model that is tested is an OLS model as in the
benchmark papers of Meese and Rogoff (1983a, 1983b,
\cite{MeeseRogoffa, MeeseRogoffb}). In this model the coefficients of
equation \ref{eq:structural} are computed without taking into account
any lagged effect.  On the other hand, a second more flexible model is
modeled to capture possible lagged effects of the macroeconomics
fundamentals. Opposed to the Meese and Rogoff papers where the authors
decided to capture the possibility of lagged terms by incorporating
exponentially smoothed autoregressive model giving a higher importance
on more recent observations I decided to apply the transfer function
models widely spread in the fields of engineering such as control
systems and electronic circuits. These models have been poorly
discussed in the field of economics with the exception of Tustin
(1957, \cite{Tustin}) that tried to
make the point for applying the models in the economic modeling field.\\
Transfer function models relates a given set of inputs to an output
variable through the following general formula

\begin{equation} \label{eq:transfer}
  Y_{t} = \mu + \frac{(\omega_{0}+\omega_{1}B^{1}+\dots+\omega_{s}B^{s})}{1-\delta_{1}B^{1}-\dots-\delta_{r}B^{r}}X_{t-b} + \varepsilon_{t}
\end{equation}

where $B$ represent the classical backshift operator, $X$ a matrix of
exogenous terms, $\mu$ the optional terms modeling the mean of the
series and $\varepsilon$ the non-captured variation in the
series. Moreover the above ratio represents the transform function of
the regressors matrix and is especially characterized by the order of
the denominator and
nominator terms. \\
$R$, the order of the denominator term, expresses the rate of the
decay pattern, where a higher term indicates a slower decay.  $S$, the
order of the nominator term, expresses the persistence of so called
unpatterned spikes, that is the persistence of exogenous effects that
are not captured in the auto-regressive decay pattern. Finally, the
$b$ term in the matrix of input represents the dead time, that is the
time it takes for the dependent variable to react to some changes in
the input matrix. This is of primary importance as it might very well
be that some shift in macroeconomic fundamentals just begins to
display effects after a certain amount of time when
the economic agents begin to perceive the change.\\
Due to the flexible nature of the latter model and the possibility to
capture exogenous regressive terms, I question whether such a model is
better suited to capture the true nature of distributed lags present
in the structural model of equation \ref{eq:structural} in comparison
to the exponentially smoothed auto-regressive model, applied by Meese
and Rogoff, with the arbitrarily chosen smoothing term of 0.95 (See
Meese and Rogoff (1983a pp. 7, \cite{MeeseRogoffa})).


\subsection{Multivariate Models}
\label{sub:multivariate}
The above discussed models rely on the exogeneity of the independent
variables. If the assumption fails, the model are biased and lead to
misleading conclusion as the independence of the sample distribution
is not guaranteed. In practice this poses an issue for the estimation
of the structural model of equation \ref{eq:structural}.

While some variables as the monetary mass and the output gap are
commonly treated as exogenous variables in the underlying monetary
models the practice suggests that such macroeconomic variables might
very well be influenced by the movements of FX-rates. This is
especially true for smaller economies as the Swiss case of 2014
showed. On the top of this, other variables such as short-term interest
rates differentials are treated as endogenous even in the underlying
monetary models and therefore require a different estimation compared
to the one outlined in the previous section.

To obviate the above issues Meese and Rogoff (1983a and 1983b,
\cite{MeeseRogoffa, MeeseRogoffb}) analyzed the structural model of
interest through a vector autoregressive model firstly introduced by
Sims (1980, \cite{Sims1980}).  In the general case the model consists
of a system of equations in the form

\begin{equation} \label{eq:VAR}
y_{t} =  \Phi_{0}+ \Phi_{1}y_{t-1} + \Phi_{2}y_{t-2} + \dots + \Phi_{n}y_{t-n} + \varepsilon_{t}
\end{equation}

where $y$ is a vector containing the endogenous variables of interest,
$\Phi_0$ is a vector of constants, $\Phi_1, \dots , \Phi_n$ are
matrices describing the effect of lagged endogenous variables on the
levels of the current variables and $\varepsilon$ captures the
equation's specific error term.  The resulting model will capture the
endogeneity present in the monetary structural model allowing for a
consistent OLS estimation as far as the error terms in the equations
are uncorrelated.

Despite the described vector autoregressive model well manages to
model the endogeneity of macroeconomic variables I question whether
the restricted form of it could yield more efficient and more reliable
estimates capturing the FX-rates and the monetary models
relation. This is especially motivated by the cointegration theory
developed by Engle and Granger (1987, \cite{EngleGranger}) and the
well known evidence of non stationary macroeconomics times series
\footnote{See for instance Gil-Alana and Robinson (1997,
  \cite{GilAlanaRobinson})}. Moreover, the importance of such method
is underlined by the evidence of Phillips (1986, \cite{Phillips}) that
set down the theoretical fundamentals showing how parameter estimates
of cointegrated series do not converge in probability and do not
converge to any non-degenerate distribution in the asymptotic case in
the case of a misspecified OLS estimate as potentially is equation
\ref{eq:VAR}.

I propose therefore a test for cointegration among the macroeconomic
and the FX-rates series based on Johansen (1991,
\cite{Johansen}) %% scrivere Johansen procedure ??%%
and I consequently estimate a vector error correction model of the
form

\begin{align}
  \Delta y_t =& \ \Pi y_{t-1} + \sum^{p-1}_{i=1} \Phi^{*} \Delta y_{t-i} + \varepsilon_t \nonumber\\
  \Phi^{*} =& \ - \sum^{p}_{i=j+1} \Phi_i, \ \  j = 1, \dots, p-1 \label{eq:VECM}\\
  \Pi =& \ -(I - \Phi_1 - \dots - \Phi_p) \nonumber\
\end{align}

where $\Pi y_{t-1}$ of equation \ref{eq:VECM} represent the error
correction term and $y_i$ and $\Phi$ refer to the variables described
in the unrestricted vector autoregressive model of \ref{eq:VAR}.

\subsection{Generalized Tree Structured Model}
\label{sub:GTS}

This section further discuss the final methodology allowing to
endogenously determine the optimal partitioned space for capturing the
non-linear structure present in the series. As outlined by Audrino and
B{\"u}hlmann, while the general idea of generalized tree structureed
models is comparable to the one of self-exciting threshold
autoregressive (SETAR) models of Tong (1983, \cite{SETAR_Tong}) and
CART models of Breiman (2017, \cite{Breiman}) the estimation of the
model and the model fit differs. In this sense, the generalized tree
structured model reselbles SETAR and CART models as it aims at
deriving the set of threshold variables that best capture the
different regimes in the time series and modeling those by local
parametric models. Nonetheless it differs from the two in the way in
which the partitioned space, and therefore the threshold splits, are
derived as well as in how the final parametric model is
fitted.\\
While the SETAR models are autoregressive self-exciting models, and
therefore search for the best partition by determining the relation
between a lagged auto-regressive variable and the theshold variable,
the generalized tree structure models are more general in the sense
that the partition space may be identified based on any external
factors independently whether they fit the parametric model or
not. Additionally, in the generalized tree structured model, the local
parametric model is not restricted to be an auto-regressive model but
is rather limited by the only restriction of being a fully specified
parametric model such that a likelihood estimation is possible. In
this regard the generalized tree structure model resembles more the
previously mentioned CART models with the difference that, while CART
models estimation technique leverages the minimization of the squared
sum of residuals, the generalized tree structured models leverages
likelihood estimation being therefore better suited for modeling
not-normal, fat-tail distributed error terms.

An exact definition of the generalized tree structured
model might be found in Audrino and B{\"u}hlmann (2001,
\cite{AudrinoBuhlmann}) and can be summarized through the follwing
steps.\\

\setlength{\leftskip}{1cm}
\setlength{\rightskip}{1cm}

\textit{Step 1:} Estimate the global parametric model over the entire
data set space $\mathcal{D}$ through the maximum likelihood method
with the optimization technique of choice.

\textit{Step 2:} Define a set of observable variables $\mathcal{S}$
that might help to proxy regimes changes or structural breaks in the
underlying parameteric model.

\textit{Step 3:} For each variable $s_j \in \mathcal{S}$
get threshold values $\overline{s}_{j,m}$, where $m$ corresponds to
the $mth$ quantile of the empirical distribution of the variable $s_j$.
Usually the $m$ are selected to be the $1/n, \ n = 1, 2, ..., 16$
empirical quantiles. Finally partition the overall data set space into
overlapping
$\mathcal{P}_{j,m} := \mathcal{P}_{j,m}^{right} \cup \
\mathcal{P}_{j,m}^{left}$ sets, where
$\mathcal{P}_{j,m}^{right}:= \{d \in \mathcal{D} \mid s_j <
\overline{s}_{j,m}\}$ and
$\mathcal{P}_{j,m}^{left} := \mathcal{P}_{j,m} \setminus
\mathcal{P}_{j,m}^{right}$.

\textit{Step 4:} Fit a parametric model on each of the defined
$\mathcal{P}_{j,m}$ sets. Define the optimal partitioned data set by
the $\mathcal{P}_{j,m}^{optimal}$ maximizing the overall maximum likelihood in
sample and note the optimal threshold variable $\overline{s}_j^{optimal}$. You
will have have obtained a new partitioned space with
$\mathcal{D} = \mathcal{P}_{j,m}^{optimal, right} \cup \
\mathcal{P}_{j,m}^{optimal, left}$.

\textit{Step 5:} Grow the tree. This means to treat each of the
partitioned data sets in the terminal nodes of the tree as the
original data set $\mathcal{D}$ and to iterate \textit{Step 2-4}. The
next optimal threshold variable is then selected by choosing the
partition displaying the highest likelihood among the selected
terminal nodes optimal partitions.\footnote{Notice, that the likelihood is
computed aggregating the residuals resulting from partitioned terminal
node data set \textit{and} the residuals of the other non-partitioned
terminal node data set.}

\textit{Step 6:} Continue to grow the tree until the
resulting partitioned space contains too little observations for a
reliable estimation of the parametric model of interest.

\textit{Step 7:} Prune the tree. This means to reduce the dimension of
the tree and the number of the partitions $\mathcal{P}$ according to
some information criteria. The information criteria should guard
against over-fitting and guarantee that a binary partition is added
just upon a sufficient increase in likelihood. The extent to which the
sufficiency condition is defined depends on the choice of information
criteria and it is left to the discretion of the end user.\\

\setlength{\leftskip}{0pt}
\setlength{\rightskip}{0pt}

Given the methodological understanding of the model I turn to the case
specific application of the model.

The generalized structured tree model is used in combination of three
parametric models. The first is the OLS structural model of
\ref{eq:structural}, the second is the vector autoregressive model
specified by \ref{eq:VAR} and the final model is the vector error
correction model of equation \ref{eq:VECM}. Where, the latter two
models are selected especially due to their increased out of sample
performance in comparison to univariate models discussed in section
\ref{sec:results}.

Due to the different parameter size of the models I
decided to modify the stopping criteria for the univariate and
multivariate models. While the stopping criteria for univariate models
will be a partitioned data set encompassing less than 40 observations
the threshold for the multivariate models was set respectively at 70 and 80
observations both to allow a larger number of observations
for the parametric fit and to allow for a robustness check of the model
by checking at the varying performance in the stopping criteria.

Moreover, for the set of observable variables $[\mathcal{S}]$ over which to search for
the optimal partition I decided to include all of the macroeconomics
fundamentals differentials outlined in the structural model described
in \ref{sub:structural} adding a time variable allowing to
capture general structural breaks in time important for the
sample data set of choice.

Finally the AICc information criteria discussed by Cavanough (1997,
\cite{Cavanough}) was chosen as the information criteria for pruning
the tree. This information criteria expands on the most known AIC
information criteria by including a correction term useful to avoid the
small sample over-fitting issue discussed in Mcquarrie (1998,
\cite{Mcquarrie}).

\begin{align}
  AIC =& \ 2k - 2ln(\hat{L}) \nonumber \\
  \label{eq:AICc} \\
  AICc =& \ AIC + \frac{2k^2 + 2k}{n - k - 1} \nonumber
\end{align}


\subsection{Forecasting Approach}
\label{sub:forecast}
The five described models are validated by looking at their ability to
forecast FX-rates out of sample in comparison to a random walk model
without drift. In this sense the 234 observations sample is splitted in a
training and a validation sample. Three fourth of the total
observations are training the five models while the remaining fourth
of the observations is used for the out of sample validation.

In comparison to Meese and Rogoff (1983b, \cite{MeeseRogoffb}) I do
not try any restricted estimation based on the theoretical monetary
models literature. I rather estimate unrestricted versions for all of
the models outlined with the exception of the vector error correction
model described in \ref{eq:VECM}, given the by product VAR model
restriction imposed by the latter.

To select the best performing monetary model I use a two step
approach. Firstly, I estimate the four parametric models according to
the most general structural model described in
\ref{eq:structural}. Secondly I perform Wald tests to check whether
there is in sample evidence for using restricted models. Based on
positive evidence I proceed by running restricted models out of
sample and look for an increased performance.

The out of sample performance of the different models is measured by
applying a rolling out-of-sample forecast in analogy to the benchmark
papers. This consists of a re-estimation of the five outlined models
for each new forecasting point. Given the decision to estimate the out
of sample model performance at one, three, six and twelve months lags
the rolling forecast technique involves a model reestimation at
respective frequencies. Important is to underline how the results of
the Wald tests are extended for each of the subsequent model
estimation in the rolling forecast. This means that given the
statistically significant evidence for the Null of a restricted model
in the first three fourth of the sample the same restricted model is
used for the subsequent model estimations.

Explicitly, this approach intends to fit a structural model containing
all of the macroeconomics series in the first 176 observations - from
September 1986 to April 2001 - and perform Wald tests to verify
whether a restricted monetary model is supported in the sample. Based
on such restults the rolling forecast method intends to estimate the
next point forecast at 1, 3, 6 and 12 month lags and shift the data
sample of one period such that a new model is estimated for the
October 1986 to May 2001 series. According to the new parametric fit,
point forecast at 1, 3, 6 and 12 months are estimated and the method
iterated until the last out of sample observation for February 2006 is
reached.

Finally, it is important to underline that as in the benchmark papers,
I allow the univariate models described in \ref{sub:univariate} a
richer set of information compared to the random walk.  Specifically,
the random walk and the multivariate model use the
$\mathscr{F}_{t-1} = {F_0, \ \dots, \ F_{t-1}}$ information set, where
$F_i$ represents a set containing all of the available information at
timepoint $i$.  By contrast the univariate models dispose of
$\mathscr{F}_t \setminus s_t$, where $s_t$ represents the FX-rate at
time point $t$.  In simple terms this means that I am going to give
the univariate models the advantage of using the actual realizations
of macroeconomics variables without the need to estimate them,
therefore outlying the possibility of poor out-of-sample fit due to
poor macroeconomic fundamentals estimation.

Based on the obtained rolling forecasts three statistics are computed
to compare the model fit.  These are the root mean squared error
(RMSE), the mean absolute error (MAE) and the mean directional
accuracy (MDA).  A particular importance is assigned to the MAE
results given Westerfield (1977, \cite{Westerfield}) that analyzed the
empirical exchange rates distribution finding evidence for FX-rates
non-normal stable-Paretian distributions with infinite variance.\\
Opposed to Meese and Rogoff (1983a and 1983b, \cite{MeeseRogoffa,
  MeeseRogoffb}) I decided to further analyze the point estimators of
the above mentioned statistics by computing a model confidence set
(MCS) as described in Hansen et al. (2011, \cite{HansenMCS}).  The
idea of the latter consists of a procedure yielding a model set,
$\mathcal{M}^*$, built to contain the best model with a chosen level
of confidence. The exact procedure is based on an equivalence test
$\delta_m$ and an elimination rule $\epsilon_m$, consistent with the
chosen confidence level. In a first step the competing models are
compared with each other. If $\delta_m$ does not support evidence for
the equal performance of the models, $\epsilon_m$ is applied, the
poorly performing models are discarded, and the general problem
reiterated until $\delta_m$ is accepted
for all of the surviving models.\\
In this paper I decided to make use of the superior predictive ability
test of Hansen (2005, \cite{HansenSPA}) and to obtain p-values for the
equal predictive hypothesis across the models according to a bootstrap
implementation of the superior predictive ability test as outlined by
Hansen et al. (2011, \cite{HansenMCS}).
