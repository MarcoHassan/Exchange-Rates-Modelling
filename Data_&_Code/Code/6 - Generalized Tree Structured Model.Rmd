---
title: "R Notebook"
output: rmarkdown::github_document
---

Step 0:

Fix the partition space according to the a-th quantile. 

Step 1:

Compute the global model fit.

Step 2:

Partition and rerun the model over all the possible different partitions. Check at improvements in the likelihood  measure.

Step 3:

Iterate.

Notice the importance of getting threshold values endogenously in comparison with the general TAR estiation procedure of Tsay or Hansen.


Useful Libraries
```{r}
## String manipulation library using regualr expressions

library(stringr) ## to make use of regular expressions to parse strings

## To handle times series .xts objects
library(xts)

## To estimate OLS
library(TSA)

## To make use of SQL queries
library(sqldf)

## To set up the generalized tree structured model
library(partykit)

## To read .txt strings
library(readr)

## Import self defined functions
source("0 - functions.R")
```

Specify a function to clean the environment and do it
```{r}
clean <- function() 
{
  ENV <- globalenv()
  ll <- ls(envir = ENV)
  ll <- ll[!(ll %in% c("data", "clean", "import", "write", "evalue", "acf_plots",
                       "adfTest", "seasonality", "suspectFrequency", "spa",
                       "logLikelihood", "partitionSpace", "fork", "bestFit", "GTS",
                       "prunedTree", "GTSEstimate", "createTree", "AICcSmall"))]
  rm(list = ll, envir = ENV)
}
```

===========
Data Import
===========

```{r}
## Insert your path here
path <- "/Users/Marco Hassan/Exchange-Rates-Modelling/Data_&_Code/Written Data/linear_pred.csv"

data <- read.csv(path)

data$time <- c(1:nrow(data))

data <- as.xts(data[,-1], order.by = as.Date(data[,1]))

closeAllConnections()
```

==========================
Tree Structured Estimation
==========================

Create the partitioned space of exogenous regressors. 

From Audrino Paper: Typically, we fix the empirical quantiles as a = i/mesh, i =
1,...,mesh - 1, where mesh determines the fineness of the grid on which we search for multivariate thresholds. Typically, we choose mesh = 8 (I chose 4, my choice is first and foremost motivated by the fact that the small sample size does not allow to have to small partitions for obvious estimation issues.)


Get exogenous variables for each of the series of interest
```{r}
## get exogenous parameters
for(country in c("CH", "UK", "JP"))
{
  assign(paste0("interest", country), 
          str_detect(colnames(data), paste0("\\D", country)) & 
         !str_detect(colnames(data), "^rw|^OLS|^var|^vecm|^M1|^str")) ## With M3
}

interestCH[length(interestCH)] = TRUE
interestJP[length(interestJP)] = TRUE
interestUK[length(interestUK)] = TRUE
```

==========================
Estimate the general model
==========================

Use OLS as the benchmark model. 

## Algorithm

```
(i) For each partition break just keep the highest likelihood function and save the structure of the partition. 

(ii) Iterate until you reach the highest number of partitions that has to be prespecified by the user.

(iii) Prune using Information criteria. 
```

Run the GTS
```{r}
## Calculate computation time
# a <- system.time(GTS(insample, max_Partition = 5))

# a[3] * 48 (out of sample observations) -> takes around 13 min. to run recompute the model at each step.

```

Create Tree
```{r}
for(country in c("CH"))
{
  data3 <- data[,get(paste0("interest", country))]
  data3 <-  data3[,c(1:(ncol(data3)-2), ncol(data3), ncol(data3)-1)]
  
  ## Define break structure and models.
  GTS(data3, max_Partition = 5)
  
  ## Save optimal partiton
  fileout <- file("optimal.txt", open = "wt")
  sink(file = fileout, append = T, type = "output")
  
  ## Pruning via AICc
  best_Partition <- prunedTree()
  
  while(sink.number() > 0)
  {sink(NULL, type="output")}
  close(fileout)
  
  opt_partition <- read_file("optimal.txt")
  
  opt_partition <- str_remove(opt_partition, "[1]")
  
  opt_partition <- str_extract(opt_partition, "[:digit:]")
  
  ## Save structure and results in a formatted way.
  png(paste0(country, "test.png"))
  createTree(iterate = max_Partition, plot = T, 
             main = paste0(country, "- Generalized Tree Structure - Full Sample\n",
                           "Optimal Partition = ", opt_partition)) ## to plot without pruning
  dev.off()
}

```

## fai il plot degli split points sulle variabli
(idea pag 14)[https://cran.r-project.org/web/packages/party/vignettes/party.pdf] 


==================
Rolling Estimation
==================

Add GTS prediction
```{r}
## trasform to data-frame to add columns
data2 <- as.data.frame(data)
## create prediction columns
for(iPred in c(1,3,6,12)) 
  { 
    country <- c("JP", "UK", "CH")
    for(jCountry in country)
    {
    data2[, paste0("GTSPred" , iPred, "_", jCountry, "US")] <- NA
    }
  } 
```

Get exogenous variables for each of the series of interest
```{r}
data2 <- data2[,-(which(colnames(data2)=="time"))]

data2$time <- c(1:nrow(data2))

## get exogenous parameters
for(country in c("CH", "UK", "JP"))
{
  assign(paste0("interest", country), 
          str_detect(colnames(data2), paste0("\\D", country)) & 
         !str_detect(colnames(data2), "^rw|^str|^var|^vecm|^GTS|^M1|^OLS|^GTS")) ## With M3
}

interestCH[length(interestCH)] = TRUE
interestJP[length(interestJP)] = TRUE
interestUK[length(interestUK)] = TRUE
```

```{r}
## in sample observations
three_fourth <- round(3/4 * nrow(data))
st <- three_fourth;
```

Rolling Prediction OLS
```{r}
for( LAG in c(12) ) 
{
  for( country in c("UK") )
  {  
    data3 <- data2[, get(paste0("interest", country))]
    data3 <-  data3[,c(1:(ncol(data3)-2), ncol(data3), ncol(data3)-1)]
    data3 <- as.xts(data3, order.by = as.Date(index(data3)))
    
    iteration = 0
    
    #Close File Connession and redirection
    if(sink.number() > 0)
    { 
      sink(NULL, type="output")
      close(fileout)
    }
    
    ## Open File where to redirect messages of the GTS fit. This will subsequently be used to generate descriptive statistics about the average numbers of splits, variables of splits. In which partitions? Etc.
  
    fileout <- file(paste0("Split/OLS/GTS_", country ,"_", LAG, ".txt"), open = "wt")
  
    sink(file = fileout, append = T, type = "output")
    
    for( iRow in 1:(nrow(data2)-st) )
    {
      iteration = iteration + 1
      
      beg <- 1 + (iRow-1)
      end <- st + (iRow -1)
      xshort <- data3[beg:end, ]
      
      ## Out of Sample Forecast
      beg <-  st + iRow
      end <- (st +iRow)+(LAG-1)
      xnew <- data3[beg:end, ]
      message(xnew[LAG, ncol(xnew)])
      xnew <- cbind(1, xnew[, -ncol(xnew)])
    
      GTS(xshort, max_Partition = 5)

      if(is.null(max_lik2))
      {
        if(xnew[LAG, match(max_lik1$optVar, names(xshort))+1] >
           max_lik1$partitioned_space[max_lik1$optVar, max_lik1$optBreak]) 
        {fcast1 <- xnew[LAG,-ncol(xnew)] %*% max_lik1$right_model} else{
          fcast1 <- xnew[LAG,-ncol(xnew)] %*%  max_lik1$left_model
        }
      
      } else{
        ## Pruning via AICc
        best_Partition <- prunedTree()
        condition <- createTree(iterate = best_Partition)
        
        ## Out of sample
        fcast1 <- GTSEstimate(xnew[LAG,])
      }
      
      ## Save the forecast
      data2[st+(LAG-1)+iRow, 
            paste0("GTSPred", LAG, "_", country, "US")] <- fcast1
      
      message(iteration)
      message(fcast1)
      if(index(tail(xshort,1)) == nrow(data2)-LAG) break 
    }
  }
}
```


Write Data to csv
```{r}
write(data2, "time_stationary_GTS_pred.csv")
```



